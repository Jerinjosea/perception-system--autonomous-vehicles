{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AWS_SingleImgDepth_(Yolov3)Notebookversion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1EU_yqgXM4XV",
        "I7qx7VfcNVgV",
        "aNulh2XvM748",
        "5crs6F9brpwq",
        "takpM5RvrupZ",
        "t51RsFcXEMHk",
        "m0jfQRuyqhHq",
        "IwvIc7iFZ__d",
        "8_2iY7HwCTlm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "580e731f611a44288e817277f61d03f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61fc2b124152430e96a8c2eac6f084b4",
              "IPY_MODEL_fbd1002ddf404d53992e18c827f8309a"
            ],
            "layout": "IPY_MODEL_49ef9f6b17bd4b24900f37234e7951c6"
          }
        },
        "61fc2b124152430e96a8c2eac6f084b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "  0%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c362cf9d428497e8cf10c3a85558ac2",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1e418eca9d248a890f465f195249b99",
            "value": 0
          }
        },
        "fbd1002ddf404d53992e18c827f8309a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc8a038214414e4c8f6a82c8679912fe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_101edabc10504730b5481307258ca473",
            "value": " 0/3 [00:00&lt;?, ?epoch/s]"
          }
        },
        "49ef9f6b17bd4b24900f37234e7951c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c362cf9d428497e8cf10c3a85558ac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1e418eca9d248a890f465f195249b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "cc8a038214414e4c8f6a82c8679912fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "101edabc10504730b5481307258ca473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EU_yqgXM4XV"
      },
      "source": [
        "#Anchor Box Calculation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2N5uR4jM32T"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class main:\n",
        "  def __init__(self,no):\n",
        "    self.num_clusters = no\n",
        "\n",
        "   \n",
        "args = main(9)\n",
        "\n",
        "'''\n",
        "Created on Feb 20, 2017\n",
        "@author: jumabek\n",
        "'''\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import argparse\n",
        "#import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import random \n",
        "import math\n",
        "\n",
        "width_in_cfg_file = 1248.\n",
        "height_in_cfg_file = 1248.\n",
        "\n",
        "def IOU(X,centroids):\n",
        "    similarities = []\n",
        "    k = len(centroids)\n",
        "    for centroid in centroids:\n",
        "        c_x,c_y,c_z = centroid\n",
        "        x,y,z = X\n",
        "        inner_vol = min(x,c_x)*min(y,c_y)*min(z,c_z)\n",
        "        similarity = inner_vol/((x*y*z+c_x*c_y*c_z)-inner_vol)\n",
        "        similarities.append(similarity) # will become (k,) shape\n",
        "    return np.array(similarities) \n",
        "\n",
        "def avg_IOU(X,centroids):\n",
        "    n,d = X.shape\n",
        "    sum = 0.\n",
        "    for i in range(X.shape[0]):\n",
        "        #note IOU() will return array which contains IoU for each centroid and X[i] // slightly ineffective, but I am too lazy\n",
        "        sum+= max(IOU(X[i],centroids)) \n",
        "    return sum/n\n",
        "\n",
        "def write_anchors_to_file(centroids,X,anchor_file):\n",
        "    f = open(anchor_file,'w')\n",
        "    \n",
        "    anchors = centroids.copy()\n",
        "    print(anchors.shape)\n",
        "\n",
        "    for i in range(anchors.shape[0]):\n",
        "        anchors[i][0]*=width_in_cfg_file\n",
        "        anchors[i][1]*=height_in_cfg_file\n",
        "        anchors[i][2]*= 5\n",
        "         \n",
        "\n",
        "    widths = anchors[:,0]+ anchors[:,1]\n",
        "    sorted_indices = np.argsort(widths)\n",
        "\n",
        "    print('Anchors = ', anchors[sorted_indices])\n",
        "        \n",
        "    for i in sorted_indices[:-1]:\n",
        "        f.write('%0.5f,%0.5f,%0.5f, '%(anchors[i,0],anchors[i,1],anchors[i,2]))\n",
        "\n",
        "    #there should not be comma after last anchor, that's why\n",
        "    f.write('%0.5f,%0.5f,%0.5f\\n'%(anchors[sorted_indices[-1:],0],anchors[sorted_indices[-1:],1],anchors[sorted_indices[-1:],2]))\n",
        "    \n",
        "    f.write('%f\\n'%(avg_IOU(X,centroids)))\n",
        "    plt.scatter(centroids.shape[0], avg_IOU(X,centroids))\n",
        "\n",
        "def kmeans(X,centroids,eps,anchor_file):\n",
        "    \n",
        "    N = X.shape[0]\n",
        "    iterations = 0\n",
        "    k,dim = centroids.shape\n",
        "    print(\"k,dim =\",k,dim)\n",
        "    prev_assignments = np.ones(N)*(-1)    \n",
        "    iter = 0\n",
        "    old_D = np.zeros((N,k))\n",
        "\n",
        "    while True:\n",
        "        D = [] \n",
        "        iter+=1           \n",
        "        for i in range(N):\n",
        "            d = 1 - IOU(X[i],centroids)\n",
        "            D.append(d)\n",
        "        D = np.array(D) # D.shape = (N,k)\n",
        "        \n",
        "        print(\"iter {}: dists = {}\".format(iter,np.sum(np.abs(old_D-D))))\n",
        "            \n",
        "        #assign samples to centroids \n",
        "        assignments = np.argmin(D,axis=1)\n",
        "        \n",
        "        if (assignments == prev_assignments).all() :\n",
        "            print(\"Centroids = \",centroids)\n",
        "            write_anchors_to_file(centroids,X,anchor_file)\n",
        "            return\n",
        "\n",
        "        #calculate new centroids\n",
        "        centroid_sums=np.zeros((k,dim),np.float)\n",
        "        for i in range(N):\n",
        "            centroid_sums[assignments[i]]+=X[i]        \n",
        "        for j in range(k):            \n",
        "            centroids[j] = centroid_sums[j]/(np.sum(assignments==j))\n",
        "        \n",
        "        prev_assignments = assignments.copy()     \n",
        "        old_D = D.copy()  \n",
        "\n",
        "    \n",
        "\n",
        "annotation_dims = []\n",
        "\n",
        "size = np.zeros((1,1,3))\n",
        "file_path = '/gdrive/My Drive/data/'\n",
        "for scene  in nusc.scene :\n",
        "  sample_token = scene['first_sample_token']\n",
        "  sample = nusc.get('sample',sample_token)\n",
        "  sensor = 'LIDAR_TOP'\n",
        "  lidar_top_data = nusc.get('sample_data', sample['data'][sensor])\n",
        "  ego_pose = nusc.get('ego_pose', lidar_top_data['ego_pose_token'])\n",
        "  for annotation in sample['anns']:\n",
        "    annotation = nusc.get('sample_annotation',annotation)\n",
        "    x,y,z = annotation['size']\n",
        "    x = float(x) / 140.\n",
        "    y = float(y) / 140.\n",
        "    z = float(z) / 5.\n",
        "    annotation_dims.append(tuple(map(float,(x,y,z))))\n",
        "#print(annotation_dims)  \n",
        "annotation_dims = np.array(annotation_dims)\n",
        "\n",
        "eps = 0.005\n",
        "\n",
        "if args.num_clusters == 0:\n",
        "    for num_clusters in range(1,11): #we make 1 through 10 clusters \n",
        "        anchor_file = 'anchors%d.txt'%(num_clusters)\n",
        "\n",
        "        indices = [ random.randrange(annotation_dims.shape[0]) for i in range(num_clusters)]\n",
        "        centroids = annotation_dims[indices]\n",
        "        kmeans(annotation_dims,centroids,eps,anchor_file)\n",
        "        print('centroids.shape', centroids.shape)\n",
        "else:\n",
        "    anchor_file = 'anchors%d.txt'%(args.num_clusters)\n",
        "    indices = [ random.randrange(annotation_dims.shape[0]) for i in range(args.num_clusters)]\n",
        "    centroids = annotation_dims[indices]\n",
        "    kmeans(annotation_dims,centroids,eps,anchor_file)\n",
        "    print('centroids.shape', centroids.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu1geigrSzaD"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "annotation_dims = []\n",
        "cat = []\n",
        "size = np.zeros((1,1,3))\n",
        "file_path = '/gdrive/My Drive/data/'\n",
        "for scene  in nusc.scene :\n",
        "  sample_token = scene['first_sample_token']\n",
        "  sample = nusc.get('sample',sample_token)\n",
        "  sensor = 'LIDAR_TOP'\n",
        "  lidar_top_data = nusc.get('sample_data', sample['data'][sensor])\n",
        "  ego_pose = nusc.get('ego_pose', lidar_top_data['ego_pose_token'])\n",
        "  for annotation in sample['anns']:\n",
        "    annotation = nusc.get('sample_annotation',annotation)\n",
        "    print(annotation['size'],annotation['category_name'])\n",
        "    x,y,z = annotation['size']\n",
        "    cat.append(annotation['category_name'])\n",
        "    #x = float(x) / 140.\n",
        "    #y = float(y) / 140.\n",
        "    #z = float(z) / 5.\n",
        "    annotation_dims.append(tuple(map(float,(x,y,z))))\n",
        "print(annotation_dims)\n",
        "d = Counter(cat)   \n",
        "print(d)\n",
        "print(annotation_dims[1])\n",
        "print(max(x[0] for x in annotation_dims))\n",
        "print(max(x[1] for x in annotation_dims))\n",
        "print(max(x[2] for x in annotation_dims))\n",
        "#print(sum(annotation_dims[:,1]))\n",
        "#print(sum(annotation_dims[:,2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vfkhkwxPhJa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5rdnxJVl4LZ"
      },
      "source": [
        "!pip3 install terminaltables\n",
        "!pip install nuscenes-devkit\n",
        "!pip install turfpy\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7qx7VfcNVgV"
      },
      "source": [
        "#data test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOKpxjoY8_8i"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "i = ListDataset(train_samples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMIOzarKLNXP"
      },
      "source": [
        "img, targets = i.__getitem__(2)\n",
        "dpi = 80\n",
        "print(img.shape)\n",
        "im_data = img.permute(1, 2, 0)\n",
        "height, width, depth = 1024, 1024, 3\n",
        "\n",
        "# What size does the figure need to be in inches to fit the image?\n",
        "figsize = width / float(dpi), height / float(dpi)\n",
        "\n",
        "# Create a figure of the right size with one axes that takes up the full figure\n",
        "fig = plt.figure(figsize=figsize)\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "# Hide spines, ticks, etc.\n",
        "ax.axis('off')\n",
        "\n",
        "# Display the image.\n",
        "ax.imshow(im_data, cmap='gray')\n",
        "\n",
        "plt.show()\n",
        "#print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LklsSzDNP6s8"
      },
      "source": [
        "img, targets = i.__getitem__(1)\n",
        "plt.imshow(  img.permute(1, 2, 0)  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNulh2XvM748"
      },
      "source": [
        "#Start off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLnS-Foc5Zav",
        "outputId": "5270bd4f-097c-4cf9-be29-bf1c09067029"
      },
      "source": [
        "%matplotlib inline\n",
        "from nuscenes.nuscenes import NuScenes \n",
        "\n",
        "nusc = NuScenes(version='v1.0-trainval', dataroot='data', verbose=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======\n",
            "Loading NuScenes tables for version v1.0-trainval...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "64386 instance,\n",
            "12 sensor,\n",
            "10200 calibrated_sensor,\n",
            "2631083 ego_pose,\n",
            "68 log,\n",
            "850 scene,\n",
            "34149 sample,\n",
            "2631083 sample_data,\n",
            "1166187 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 36.724 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 9.4 seconds.\n",
            "======\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTQzGTHQ0FfF"
      },
      "source": [
        "#import wandb\n",
        "#wandb.init()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH_zIRd2IP1j"
      },
      "source": [
        "\n",
        "from __future__ import division\n",
        "\n",
        "from terminaltables import AsciiTable\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5crs6F9brpwq"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q0mYRDmhNzp"
      },
      "source": [
        "#Angle Decoder\n",
        "\n",
        "def angle_decoder(r):\n",
        "  teta1 = torch.asin(2*r[0] - 1)\n",
        "  teta2 = torch.acos(2*r[1] - 1)\n",
        "  teta = 0\n",
        "  if 2*r[0] - 1 >= 0 and 2*r[1] - 1 >= 0:\n",
        "    teta = (teta1+teta2)/2\n",
        "  elif 2*r[0] - 1 >= 0 and 2*r[1] - 1 < 0:\n",
        "    teta = (math.pi-teta1+teta2)/2\n",
        "  elif 2*r[0] - 1 < 0 and 2*r[1] - 1 <= 0:\n",
        "    teta = (math.pi - teta1+2*math.pi -teta2)/2\n",
        "  elif 2*r[0] - 1 < 0 and 2*r[1] - 1 > 0:\n",
        "    teta = (2*math.pi + teta1+2*math.pi - teta2)/2\n",
        "  return teta"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8QQyIXmraeD"
      },
      "source": [
        "#Utils\n",
        "\n",
        "from __future__ import division\n",
        "import math\n",
        "import time\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "\n",
        "def to_cpu(tensor):\n",
        "    return tensor.detach().cpu()\n",
        "\n",
        "def rotate_around_point(point, radians, origin=(0, 0)):\n",
        "  \"\"\"Rotate a point around a given point.\n",
        "  \n",
        "  I call this the \"low performance\" version since it's recalculating\n",
        "  the same values more than once [cos(radians), sin(radians), x-ox, y-oy).\n",
        "  It's more readable than the next function, though.\n",
        "  \"\"\"\n",
        "  x, y = point\n",
        "  ox, oy = origin\n",
        "\n",
        "  qx = ox + math.cos(radians) * (x - ox) + math.sin(radians) * (y - oy)\n",
        "  qy = oy + -math.sin(radians) * (x - ox) + math.cos(radians) * (y - oy)\n",
        "\n",
        "  return qx.item(), qy.item()\n",
        "\n",
        "def load_classes(path):\n",
        "    \"\"\"\n",
        "    Loads class labels at 'path'\n",
        "    \"\"\"\n",
        "    fp = open(path, \"r\")\n",
        "    names = fp.read().split(\"\\n\")[:-1]\n",
        "    return names\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def rescale_boxes(boxes, current_dim, original_shape):\n",
        "    \"\"\" Rescales bounding boxes to the original shape \"\"\"\n",
        "    orig_h, orig_w = original_shape\n",
        "    # The amount of padding that was added\n",
        "    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n",
        "    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n",
        "    # Image height and width after padding is removed\n",
        "    unpad_h = current_dim - pad_y\n",
        "    unpad_w = current_dim - pad_x\n",
        "    # Rescale bounding boxes to dimension of original image\n",
        "    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n",
        "    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n",
        "    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n",
        "    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
        "    # Arguments\n",
        "        tp:    True positives (list).\n",
        "        conf:  Objectness value from 0-1 (list).\n",
        "        pred_cls: Predicted object classes (list).\n",
        "        target_cls: True object classes (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort by objectness\n",
        "    i = np.argsort(-conf)\n",
        "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes = np.unique(target_cls)\n",
        "\n",
        "    # Create Precision-Recall curve and compute AP for each class\n",
        "    ap, p, r = [], [], []\n",
        "    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n",
        "        i = pred_cls == c\n",
        "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
        "        n_p = i.sum()  # Number of predicted objects\n",
        "\n",
        "        if n_p == 0 and n_gt == 0:\n",
        "            continue\n",
        "        elif n_p == 0 or n_gt == 0:\n",
        "            ap.append(0)\n",
        "            r.append(0)\n",
        "            p.append(0)\n",
        "        else:\n",
        "            # Accumulate FPs and TPs\n",
        "            fpc = (1 - tp[i]).cumsum()\n",
        "            tpc = (tp[i]).cumsum()\n",
        "\n",
        "            # Recall\n",
        "            recall_curve = tpc / (n_gt + 1e-16)\n",
        "            r.append(recall_curve[-1])\n",
        "\n",
        "            # Precision\n",
        "            precision_curve = tpc / (tpc + fpc)\n",
        "            p.append(precision_curve[-1])\n",
        "\n",
        "            # AP from recall-precision curve\n",
        "            ap.append(compute_ap(recall_curve, precision_curve))\n",
        "\n",
        "    # Compute F1 score (harmonic mean of precision and recall)\n",
        "    p, r, ap = np.array(p), np.array(r), np.array(ap)\n",
        "    f1 = 2 * p * r / (p + r + 1e-16)\n",
        "\n",
        "    return p, r, ap, f1, unique_classes.astype(\"int32\")\n",
        "\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list).\n",
        "        precision: The precision curve (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
        "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "\n",
        "def get_batch_statistics(outputs, targets, iou_threshold):\n",
        "    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n",
        "    batch_metrics = []\n",
        "    for sample_i in range(len(outputs)):\n",
        "\n",
        "        if outputs[sample_i] is None:\n",
        "            continue\n",
        "\n",
        "        output = outputs[sample_i]\n",
        "        pred_boxes = output[:, :8]\n",
        "        pred_scores = output[:, 8]\n",
        "        pred_labels = output[:, -1]\n",
        "\n",
        "        true_positives = np.zeros(pred_boxes.shape[0])\n",
        "\n",
        "        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n",
        "        target_labels = annotations[:, 0] if len(annotations) else []\n",
        "        if len(annotations):\n",
        "            detected_boxes = []\n",
        "            target_boxes = annotations[:, 1:]\n",
        "\n",
        "            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n",
        "\n",
        "                # If targets are found break\n",
        "                if len(detected_boxes) == len(annotations):\n",
        "                    break\n",
        "\n",
        "                # Ignore if label is not one of the target labels\n",
        "                if pred_label not in target_labels:\n",
        "                    continue\n",
        "\n",
        "                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n",
        "                if iou >= iou_threshold and box_index not in detected_boxes:\n",
        "                    true_positives[pred_i] = 1\n",
        "                    detected_boxes += [box_index]\n",
        "        batch_metrics.append([true_positives, pred_scores, pred_labels])\n",
        "    return batch_metrics\n",
        "\n",
        "\n",
        "def bbox_wh_iou(wh1, wh2):\n",
        "    wh2 = wh2.t()\n",
        "    w1, l1, h1 = wh1[0], wh1[1] ,wh1[2]\n",
        "    w2, l2, h2 = wh2[0], wh2[1] ,wh1[2]\n",
        "    inter_area = torch.min(w1, w2) * torch.min(l1, l2) * torch.min(h1, h2)\n",
        "    union_area = (w1 * l1 * h1 + 1e-16) + (w2 * l2 * h2) - inter_area\n",
        "    return inter_area / union_area\n",
        "\n",
        "\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Returns the IoU of two bounding boxes\n",
        "    \"\"\"\n",
        "    #print(\"box shapes\",box1.shape,box2.shape)\n",
        "    iou_scores = []\n",
        "    # Transform from center and width to exact coordinates\n",
        "    b1_x1, b1_x3 = box1[:,0] - box1[:,3] / 2, box1[:,0] + box1[:,3] / 2\n",
        "    b1_y1, b1_y3 = box1[:,1] - box1[:,4] / 2, box1[:,1] + box1[:,4] / 2\n",
        "    b1_x2, b1_x4 = b1_x3, b1_x1\n",
        "    b1_y2, b1_y4 = b1_y1, b1_y3\n",
        "    #b1_z1, b1_z3 = box1[2] - box1[5] / 2, box1[2] + box1[5] / 2\n",
        "    b2_x1, b2_x3 = box2[:,0] - box2[:,3] / 2, box2[:,0] + box2[:,3] / 2\n",
        "    b2_x2, b2_x4 = b2_x3, b2_x1\n",
        "    b2_y1, b2_y3 = box2[:,1] - box2[:,4] / 2, box2[:,1] + box2[:,4] / 2\n",
        "    b2_y2, b2_y4 = b2_y1, b2_y3\n",
        "    #b2_z1, b2_z3 = box2[2] - box2[5] / 2, box2[2] + box2[5] / 2\n",
        "\n",
        "    rotation1 = [angle_decoder(r[6:8]) for r in box1]\n",
        "    rotation2 = [angle_decoder(r[6:8]) for r in box2]\n",
        "    \n",
        "\n",
        "\n",
        "    if box1.shape == box2.shape:\n",
        "      #print(\"in 1\")\n",
        "      for x11,x12,x13,x14,y11,y12,y13,y14,x21,x22,x23,x24,y21,y22,y23,y24,r1,r2, b1,b2 in zip(b1_x1,b1_x2,b1_x3,b1_x4, b1_y1,b1_y2,b1_y3,b1_y4, b2_x1,b2_x2,b2_x3,b2_x4, b2_y1,b2_y2,b2_y3,b2_y4, rotation1,rotation2, box1,box2):\n",
        "        p1 = Polygon([rotate_around_point((x11, y11), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x12, y12), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x13, y13), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x14, y14), r1, (b1[0], b1[1]))])\n",
        "        p2 = Polygon([rotate_around_point((x21, y21), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x22, y22), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x23, y23), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x24, y24), r2, (b2[0], b2[1]))])\n",
        "        p3 = p1.intersection(p2)\n",
        "        inter_area = p3.area\n",
        "        b1_area = p1.area\n",
        "        b2_area = p2.area\n",
        "\n",
        "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "      return torch.Tensor(iou_scores).to(device)\n",
        "    else:\n",
        "      #print(\"in 2\")\n",
        "      x11,x12,x13,x14, y11,y12,y13,y14 = b1_x1[0],b1_x2[0],b1_x3[0],b1_x4[0], b1_y1[0],b1_y2[0],b1_y3[0],b1_y4[0]\n",
        "      r1, b1 = rotation1[0], box1[0]\n",
        "      for x21,x22,x23,x24,y21,y22,y23,y24,r2 ,b2 in zip(b2_x1,b2_x2,b2_x3,b2_x4, b2_y1,b2_y2,b2_y3,b2_y4, rotation2, box2):\n",
        "        p1 = Polygon([rotate_around_point((x11, y11), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x12, y12), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x13, y13), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x14, y14), r1, (b1[0], b1[1]))])\n",
        "        p2 = Polygon([rotate_around_point((x21, y21), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x22, y22), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x23, y23), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x24, y24), r2, (b2[0], b2[1]))])\n",
        "        p3 = p1.intersection(p2)\n",
        "        inter_area = p3.area\n",
        "        b1_area = p1.area\n",
        "        b2_area = p2.area\n",
        "\n",
        "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "        iou_scores.append(iou)\n",
        "        #print(\"iou = \",iou,\"iou_scores.shape :\",len(iou_scores))\n",
        "\n",
        "      return torch.Tensor(iou_scores).to(device)\n",
        "\n",
        "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.5):\n",
        "    \"\"\"\n",
        "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
        "    Non-Maximum Suppression to further filter detections.\n",
        "    Returns detections with shape:\n",
        "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
        "    \"\"\"\n",
        "\n",
        "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "    output = [None for _ in range(len(prediction))]\n",
        "    for image_i, image_pred in enumerate(prediction):\n",
        "        # Filter out confidence scores below threshold\n",
        "        image_pred = image_pred[image_pred[:, 8] >= conf_thres]\n",
        "        # If none are remaining => process next image\n",
        "        if not image_pred.size(0):\n",
        "            continue\n",
        "        # Object confidence times class confidence\n",
        "        score = image_pred[:, 8] * image_pred[:, 9:].max(1)[0]\n",
        "        # Sort by it\n",
        "        image_pred = image_pred[(-score).argsort()]\n",
        "        class_confs, class_preds = image_pred[:, 9:].max(1, keepdim=True)\n",
        "        detections = torch.cat((image_pred[:, :9], class_confs.float(), class_preds.float()), 1)\n",
        "        # Perform non-maximum suppression\n",
        "        keep_boxes = []\n",
        "        while detections.size(0):\n",
        "            #print(\"bbox :\",bbox_iou(detections[0, :8].unsqueeze(0), detections[:, :8]))\n",
        "            large_overlap = bbox_iou(detections[0, :8].unsqueeze(0), detections[:, :8]) > nms_thres\n",
        "            #print(\"detection.size:\",detections.size())\n",
        "            label_match = detections[0, -1] == detections[:, -1]\n",
        "            label_match = label_match.cuda()\n",
        "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
        "            invalid = large_overlap & label_match\n",
        "            weights = detections[invalid, 8:9]\n",
        "            # Merge overlapping bboxes by order of confidence\n",
        "            detections[0, :8] = (weights * detections[invalid, :8]).sum(0) / weights.sum()\n",
        "            keep_boxes += [detections[0]]\n",
        "            detections = detections[~invalid]\n",
        "        if keep_boxes:\n",
        "            output[image_i] = torch.stack(keep_boxes)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
        "    ByteTensor = torch.cuda.BoolTensor if pred_boxes.is_cuda else torch.BoolTensor\n",
        "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
        "\n",
        "    nB = pred_boxes.size(0)\n",
        "    nA = pred_boxes.size(1)\n",
        "    nC = pred_cls.size(-1)\n",
        "    nG = pred_boxes.size(2)\n",
        "    \n",
        "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
        "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
        "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tz = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tl = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tr1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tr2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
        "    \n",
        "    target_boxes = target[:, 2:10]\n",
        "    target_boxes[:, :2] = target_boxes[:, :2] * nG\n",
        "    target_boxes[:, 3:5] = target_boxes[:, 3:5] * nG\n",
        "    target_boxes[:, 5] = target_boxes[:, 5] * 5\n",
        "\n",
        "    gxy = target_boxes[:, :2]\n",
        "    gwlh = target_boxes[:, 3:6]\n",
        "\n",
        "    ious = torch.stack([bbox_wh_iou(anchor, gwlh) for anchor in anchors]) #here\n",
        "    try:\n",
        "      best_ious, best_n = ious.max(0)\n",
        "      #print(\"ious shape :\",ious.shape)\n",
        "      #print(\"ious :\", ious)\n",
        "      #print(\"Best_ious shape :\", best_ious.shape)\n",
        "      #print(\"Best_ious :\", best_ious)\n",
        "      #print(\"Best_n shape :\", best_n.shape)\n",
        "      #print(\"Best_n :\", best_n)\n",
        "\n",
        "\n",
        "    except:\n",
        "      best_ious, best_n = torch.LongTensor(0), torch.LongTensor(0)\n",
        "      #tconf = obj_mask.float()\n",
        "      #return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf\n",
        "      pass\n",
        "\n",
        "    b, target_labels = target[:, :2].long().t()\n",
        "    \n",
        "    gx, gy = gxy.t()\n",
        "    gw, gl, gh = gwlh.t()\n",
        "    gi, gj = gxy.long().t()\n",
        "    \n",
        "    # Set masks\n",
        "    obj_mask[b, best_n, gj, gi] = 1\n",
        "    noobj_mask[b, best_n, gj, gi] = 0\n",
        "\n",
        "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
        "    for i, anchor_ious in enumerate(ious.t()):\n",
        "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
        "\n",
        "    # Coordinates\n",
        "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
        "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
        "    tz[b, best_n, gj, gi] = target_boxes[:,2]\n",
        "    # Width and height\n",
        "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
        "    tl[b, best_n, gj, gi] = torch.log(gl / anchors[best_n][:, 1] + 1e-16)\n",
        "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 2] + 1e-16)\n",
        "    \n",
        "    tr1[b, best_n, gj, gi] = target_boxes[:,6]\n",
        "    tr2[b, best_n, gj, gi] = target_boxes[:,7]\n",
        "    # One-hot encoding of label\n",
        "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
        "    # Compute label correctness and iou at best anchor\n",
        "    try:\n",
        "      class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
        "    except:\n",
        "      pass\n",
        "    tconf = obj_mask.float()\n",
        "    target_boxes[:, :2] = target_boxes[:, :2] / nG\n",
        "    target_boxes[:, 3:5] = target_boxes[:, 3:5] / nG\n",
        "    target_boxes[:, 5] = target_boxes[:, 5] / 5\n",
        "    return class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfNuZWdMrK2w"
      },
      "source": [
        "#Parse_config\n",
        "\n",
        "def parse_model_config(path):\n",
        "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
        "    file = open(path, 'r')\n",
        "    lines = file.read().split('\\n')\n",
        "    lines = [x for x in lines if x and not x.startswith('#')]\n",
        "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
        "    module_defs = []\n",
        "    for line in lines:\n",
        "        if line.startswith('['): # This marks the start of a new block\n",
        "            module_defs.append({})\n",
        "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
        "            if module_defs[-1]['type'] == 'convolutional':\n",
        "                module_defs[-1]['batch_normalize'] = 0\n",
        "        else:\n",
        "            key, value = line.split(\"=\")\n",
        "            value = value.strip()\n",
        "            module_defs[-1][key.rstrip()] = value.strip()\n",
        "\n",
        "    return module_defs\n",
        "\n",
        "def parse_data_config(path):\n",
        "    \"\"\"Parses the data configuration file\"\"\"\n",
        "    options = dict()\n",
        "    options['gpus'] = '0,1,2,3'\n",
        "    options['num_workers'] = '10'\n",
        "    with open(path, 'r') as fp:\n",
        "        lines = fp.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line == '' or line.startswith('#'):\n",
        "            continue\n",
        "        key, value = line.split('=')\n",
        "        options[key.strip()] = value.strip()\n",
        "    return options\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xUCTALNYmpR"
      },
      "source": [
        "#Draw\n",
        "def draw(targets):\n",
        "  fig = plt.figure(figsize=(8,8))\n",
        "  ax = fig.add_subplot(111)\n",
        "  for target in targets:\n",
        "    rotation = angle_decoder(target[6:8]) + math.pi/2\n",
        "    length = target[3]\n",
        "    width = target[4]\n",
        "    x_temp, y_temp = rotate_around_point((target[0],-target[1]), -(rotation), origin=(target[0]-width/2, -target[1] - length/2))\n",
        "    x_offset, y_offset = x_temp - target[0], y_temp + target[1]\n",
        "    rectas = patches.Rectangle(xy=((target[0]-width/2) - x_offset, (-target[1] - length/2) - y_offset) ,width=width, height=length, angle = (rotation)*180/math.pi, linewidth=1, color='blue', fill=False)\n",
        "    ax.add_patch(rectas)\n",
        "    ax.scatter(target[0], -target[1], color = 'red', s=10)\n",
        "  ax.scatter(0.5, -0.5)\n",
        "  plt.xlim(0, 1)\n",
        "  plt.ylim(-1,0)\n",
        "  plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0phLTHEsL0P"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pyquaternion import Quaternion\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def pad_to_square(img, pad_value):\n",
        "    c, h, w = img.shape\n",
        "    dim_diff = np.abs(h - w)\n",
        "    # (upper / left) padding and (lower / right) padding\n",
        "    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
        "    # Determine padding\n",
        "    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n",
        "    # Add padding\n",
        "    img = F.pad(img, pad, \"constant\", value=pad_value)\n",
        "\n",
        "    return img, pad\n",
        "\n",
        "\n",
        "def resize(image, size):\n",
        "    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_resize(images, min_size=288, max_size=448):\n",
        "    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]\n",
        "    images = F.interpolate(images, size=new_size, mode=\"nearest\")\n",
        "    return images\n",
        "\n",
        "\n",
        "class ImageFolder(Dataset):\n",
        "    def __init__(self, folder_path, img_size=1248):\n",
        "        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[index % len(self.files)]\n",
        "        # Extract image as PyTorch tensor\n",
        "        img = transforms.ToTensor()(Image.open(img_path))\n",
        "        # Pad to square resolution\n",
        "        img, _ = pad_to_square(img, 0)\n",
        "        # Resize\n",
        "        img = resize(img, self.img_size)\n",
        "\n",
        "        return img_path, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    def __init__(self, sample_mapping, img_size=1248, augment=True, multiscale=False, normalized_labels=False, max_height=5, max_length=20, max_width=20):\n",
        "       \n",
        "        self.img_size = img_size\n",
        "        self.max_objects = 100\n",
        "        self.augment = augment\n",
        "        self.multiscale = multiscale\n",
        "        self.normalized_labels = normalized_labels\n",
        "        self.min_size = self.img_size - 3 * 32\n",
        "        self.max_size = self.img_size + 3 * 32\n",
        "        self.batch_count = 0\n",
        "        self.sample_mapping = sample_mapping\n",
        "        self.file_path = 'data/'\n",
        "        \n",
        "\n",
        "        #get categories\n",
        "        self.categories = []\n",
        "        with open('data/v1.0-trainval/category.json') as f:\n",
        "          data = json.load(f)\n",
        "        for d in data:\n",
        "          self.categories.append(d['name'])\n",
        "        self.num_category = len(self.categories)\n",
        "        \n",
        "        #important items for tweking\n",
        "        self.max_height = max_height\n",
        "        self.max_width = max_width\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def quaternion_yaw(self, q: Quaternion) -> float:\n",
        "      \"\"\"\n",
        "      Calculate the yaw angle from a quaternion.\n",
        "      See https://en.wikipedia.org/wiki/Conversion_between_quaternions_and_Euler_angles.\n",
        "      :param q: Quaternion of interest.\n",
        "      :return: Yaw angle in radians.\n",
        "      \"\"\"\n",
        "\n",
        "      a = 2.0 * (q[0] * q[3] + q[1] * q[2])\n",
        "      b = 1.0 - 2.0 * (q[2] ** 2 + q[3] ** 2)\n",
        "\n",
        "      return np.arctan2(a, b)\n",
        "\n",
        "    def rotate_around_point_lowperf(self, point, radians, origin=(0, 0)):\n",
        "      \"\"\"Rotate a point around a given point.\n",
        "      \n",
        "      I call this the \"low performance\" version since it's recalculating\n",
        "      the same values more than once [cos(radians), sin(radians), x-ox, y-oy).\n",
        "      It's more readable than the next function, though.\n",
        "      \"\"\"\n",
        "      x, y = point\n",
        "      ox, oy = origin\n",
        "\n",
        "      qx = ox + math.cos(radians) * (x - ox) + math.sin(radians) * (y - oy)\n",
        "      qy = oy + -math.sin(radians) * (x - ox) + math.cos(radians) * (y - oy)\n",
        "\n",
        "      return qx, qy\n",
        "\n",
        "    def convert_to_top_corner(self, point):\n",
        "      \"\"\"Convert the position with respect to the top left corner\"\"\"\n",
        "      point[0] = self.max_length + point[0]\n",
        "      point[1] = self.max_width - point[1]\n",
        "      return point\n",
        "\n",
        "    def check_cameraregion(self,coordinates,cameras,sample,verbose = False):\n",
        "      \"\"\"To check if the coordinate of the object lies in blacked out camera region\n",
        "       and if so return flag as 0\"\"\"\n",
        "      if verbose:\n",
        "        print(\"Start of Checking for the point :\",coordinates)\n",
        "      angle = 0\n",
        "      flag = False\n",
        "      for camera in cameras:\n",
        "        flag = False\n",
        "        sample_data = nusc.get('sample_data',sample['data'][camera])\n",
        "        sensor = nusc.get('calibrated_sensor',sample_data['calibrated_sensor_token'])\n",
        "        if camera == 'CAM_BACK':\n",
        "          angle = 55/180 * math.pi\n",
        "        else :\n",
        "          angle = 35/180 * math.pi\n",
        "        if verbose:\n",
        "          print(\"Sensor before rotation:\", sensor['translation'][0:2])\n",
        "        x, y = self.rotate_around_point_lowperf(sensor['translation'][0:2], -math.pi/2)\n",
        "        if verbose:\n",
        "          print(\"Sensor after rotation:\", x,y)\n",
        "        rotation = self.quaternion_yaw(sensor['rotation']) +math.pi\n",
        "        vl = coordinates[1] - y - math.tan(rotation+angle) * (coordinates[0]-x)\n",
        "        vr = coordinates[1] - y - math.tan(rotation-angle) * (coordinates[0]-x)\n",
        "        if verbose:\n",
        "          print(camera,\"Angles :\",(rotation+angle)*180/math.pi,(rotation-angle)*180/math.pi)\n",
        "        if (rotation + angle >= math.pi/2 and rotation + angle <= math.pi*3/2):\n",
        "          if vl >= 0:\n",
        "            flag = True\n",
        "        else:\n",
        "          if vl <= 0:\n",
        "            flag = True\n",
        "        if flag:\n",
        "          if (rotation - angle >= math.pi/2 and rotation - angle <= math.pi*3/2):\n",
        "            if vr <= 0:\n",
        "              flag = True\n",
        "              break\n",
        "            else:\n",
        "              flag = False\n",
        "          else:\n",
        "            if vr >= 0:\n",
        "              flag = True\n",
        "              break\n",
        "            else:\n",
        "              flag = False\n",
        "      return flag\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        token = self.sample_mapping[index]\n",
        "        my_sample = nusc.get('sample',token)\n",
        "        \n",
        "        # ---------\n",
        "        #  Image\n",
        "        # ---------\n",
        "\n",
        "        front = ['CAM_FRONT_LEFT','CAM_FRONT','CAM_FRONT_RIGHT']\n",
        "        back = ['CAM_BACK_RIGHT','CAM_BACK','CAM_BACK_LEFT']\n",
        "\n",
        "        camera = []\n",
        "        blackout_cameras = []\n",
        "\n",
        "        #Augmentation using camera blackout\n",
        "        if self.augment:\n",
        "          if np.random.random() < 0.3:\n",
        "            camera = front + back\n",
        "            numbers = [1,2,3,4,5]\n",
        "            number = random.choice(numbers)\n",
        "            blackout_cameras = random.sample(camera,number)\n",
        "\n",
        "        #Camera image stitching and applying blackout from the selected cameras\n",
        "        for f,b in zip(front,back):\n",
        "          sensorf = nusc.get('sample_data',my_sample['data'][f])\n",
        "          sensorb = nusc.get('sample_data',my_sample['data'][b])\n",
        "\n",
        "          if f == 'CAM_FRONT_LEFT' and b == 'CAM_BACK_RIGHT':\n",
        "            image_dataf = transforms.ToTensor()(Image.open(self.file_path + sensorf['filename']).convert('RGB'))\n",
        "            if 'CAM_FRONT_LEFT' in blackout_cameras:\n",
        "              image_dataf = torch.rand(image_dataf.shape)\n",
        "            image_datab = transforms.ToTensor()(Image.open(self.file_path + sensorb['filename']).convert('RGB'))\n",
        "            if 'CAM_BACK_RIGHT' in blackout_cameras:\n",
        "              image_datab = torch.rand(image_dataf.shape)\n",
        "          else:\n",
        "            data = transforms.ToTensor()(Image.open(self.file_path + sensorf['filename']).convert('RGB'))\n",
        "            if f in blackout_cameras:\n",
        "              data = torch.rand(data.shape)\n",
        "            image_dataf = torch.cat((image_dataf,data),2)\n",
        "            data = transforms.ToTensor()(Image.open(self.file_path + sensorb['filename']).convert('RGB'))\n",
        "            if b in blackout_cameras:\n",
        "              data = torch.rand(data.shape)\n",
        "            image_datab = torch.cat((image_datab,data),2)\n",
        "        #fliping the bottem image for more consistency\n",
        "        image_datab = torch.flip(image_datab, [-1])\n",
        "        #concatinating the top and bottem image\n",
        "        image_data = torch.cat((image_dataf,image_datab),1)\n",
        "        #resizeing the image to square\n",
        "        image_data, _ = pad_to_square(image_data,0)\n",
        "\n",
        "        \n",
        "        # ---------\n",
        "        #  Label\n",
        "        # ---------\n",
        "\n",
        "        l_factor, w_factor, h_factor = (self.max_length, self.max_width, self.max_height) if self.normalized_labels else (1, 1, 1)\n",
        "        targets = None\n",
        "\n",
        "\n",
        "        annos_list = my_sample['anns']\n",
        "        converted_anotations = []\n",
        "        #ego pose\n",
        "        sensor = 'LIDAR_TOP'\n",
        "        lidar_top_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
        "        ego_pose = nusc.get('ego_pose', lidar_top_data['ego_pose_token'])\n",
        "        ego_yaw = self.quaternion_yaw(ego_pose['rotation']) - math.pi/2\n",
        "\n",
        "        boxes = []\n",
        "        t=[]\n",
        "        original_ego_yaw = ego_yaw + math.pi/2 #converting back to original value\n",
        "\n",
        "        for annos in annos_list:\n",
        "          annotation = nusc.get('sample_annotation', annos)\n",
        "          vis = nusc.get('visibility',annotation['visibility_token'])\n",
        "          vis = vis['level'][1:].split(\"-\")\n",
        "          if int(vis[1]) <= 40 :\n",
        "            continue\n",
        "          #print(vis)\n",
        "          box = []\n",
        "\n",
        "          #xyz\n",
        "          flag = False\n",
        "          cordinates = [annotation['translation'][i] - ego_pose['translation'][i] for i in range(3)]\n",
        "          cordinates[0], cordinates[1] = self.rotate_around_point_lowperf(cordinates[:2], ego_yaw, origin=(0, 0))\n",
        "          if self.augment :\n",
        "            flag = self.check_cameraregion(cordinates,blackout_cameras,my_sample)\n",
        "          cordinates = self.convert_to_top_corner(cordinates)\n",
        "          if cordinates[0] > 2*self.max_width or cordinates[0] < 0 or cordinates[1] > 2*self.max_length or cordinates[1] < 0 or flag:# or (self.augment and self.check_cameraregion() == 0):\n",
        "            continue\n",
        "\n",
        "          #whl\n",
        "          size = annotation['size']\n",
        "\n",
        "          #angle r1, r2\n",
        "            #converting to relative angle (0-360)\n",
        "          rotation_yaw = self.quaternion_yaw(annotation['rotation']) - original_ego_yaw\n",
        "          if rotation_yaw < 0:\n",
        "            rotation_yaw += math.pi*2\n",
        "          r1 = (1 + math.sin(rotation_yaw))/2\n",
        "          r2 = (1 + math.cos(rotation_yaw))/2\n",
        "\n",
        "          #category\n",
        "          category_index = self.categories.index(annotation['category_name'])\n",
        "\n",
        "          #Appending to Box\n",
        "          box.append(category_index)\n",
        "          for i,j in zip(cordinates, [self.max_width*2, self.max_length*2, self.max_height]):\n",
        "            box.append(i/j)\n",
        "          for i,j in zip(size, [self.max_width*2, self.max_length*2, self.max_height]):\n",
        "            box.append(i/j)\n",
        "          box.append(r1)\n",
        "          box.append(r2)\n",
        "\n",
        "          #Appending to Boxes\n",
        "          boxes.append(box)\n",
        "          t.append(annos)\n",
        "\n",
        "        boxes = torch.Tensor(boxes)\n",
        "\n",
        "        targets = torch.zeros((len(boxes), 10))\n",
        "        if len(boxes)> 0:\n",
        "          targets[:, 1:] = boxes\n",
        "\n",
        "        # Apply augmentations\n",
        "        #if self.augment:\n",
        "        #  image_data, targets = horisontal_flip(image_data, targets, Verbose = True)\n",
        "        #    if np.random.random() < 0.5:\n",
        "        return image_data, targets\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        imgs, targets = list(zip(*batch))\n",
        "        # Remove empty placeholder targets\n",
        "        targets = [boxes for boxes in targets if boxes is not None]\n",
        "        # Add sample index to targets\n",
        "        for i, boxes in enumerate(targets):\n",
        "            targets[i][:, 0] = i\n",
        "        targets = torch.cat(targets, 0)\n",
        "        \"\"\"\n",
        "        # Selects new image size every tenth batch\n",
        "        if self.multiscale and self.batch_count % 10 == 0:\n",
        "            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n",
        "        \"\"\"\n",
        "        # Resize images to input shape\n",
        "        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n",
        "        self.batch_count += 1\n",
        "        \n",
        "        return imgs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_mapping)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "takpM5RvrupZ"
      },
      "source": [
        "#MODEL.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDDUlseGjmRJ"
      },
      "source": [
        "#MODEL\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "\n",
        "def create_modules(module_defs):\n",
        "    \"\"\"\n",
        "    Constructs module list of layer blocks from module configuration in module_defs\n",
        "    \"\"\"\n",
        "    hyperparams = module_defs.pop(0)\n",
        "    output_filters = [int(hyperparams[\"channels\"])]\n",
        "    module_list = nn.ModuleList()\n",
        "    for module_i, module_def in enumerate(module_defs):\n",
        "        modules = nn.Sequential()\n",
        "\n",
        "        if module_def[\"type\"] == \"convolutional\":\n",
        "            bn = int(module_def[\"batch_normalize\"])\n",
        "            filters = int(module_def[\"filters\"])\n",
        "            kernel_size = int(module_def[\"size\"])\n",
        "            pad = (kernel_size - 1) // 2\n",
        "            modules.add_module(\n",
        "                f\"conv_{module_i}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=output_filters[-1],\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=int(module_def[\"stride\"]),\n",
        "                    padding=pad,\n",
        "                    bias=not bn,\n",
        "                ),\n",
        "            )\n",
        "            if bn:\n",
        "                modules.add_module(f\"batch_norm_{module_i}\", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n",
        "            if module_def[\"activation\"] == \"leaky\":\n",
        "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
        "\n",
        "        elif module_def[\"type\"] == \"maxpool\":\n",
        "            kernel_size = int(module_def[\"size\"])\n",
        "            stride = int(module_def[\"stride\"])\n",
        "            if kernel_size == 2 and stride == 1:\n",
        "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
        "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n",
        "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
        "\n",
        "        elif module_def[\"type\"] == \"upsample\":\n",
        "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
        "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
        "\n",
        "        elif module_def[\"type\"] == \"route\":\n",
        "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
        "            filters = sum([output_filters[1:][i] for i in layers])\n",
        "            modules.add_module(f\"route_{module_i}\", EmptyLayer())\n",
        "\n",
        "        elif module_def[\"type\"] == \"shortcut\":\n",
        "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
        "            modules.add_module(f\"shortcut_{module_i}\", EmptyLayer())\n",
        "\n",
        "        elif module_def[\"type\"] == \"yolo\":\n",
        "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
        "            # Extract anchors\n",
        "            anchors = [float(x) for x in module_def[\"anchors\"].split(\",\")]\n",
        "            anchors = [(anchors[i], anchors[i + 1], anchors[i + 2]) for i in range(0, len(anchors), 3)]\n",
        "            anchors = [anchors[i] for i in anchor_idxs]\n",
        "            num_classes = int(module_def[\"classes\"])\n",
        "            img_size = int(hyperparams[\"height\"])\n",
        "            # Define detection layer\n",
        "            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n",
        "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
        "        # Register module list and number of output filters\n",
        "        module_list.append(modules)\n",
        "        output_filters.append(filters)\n",
        "\n",
        "    return hyperparams, module_list\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
        "\n",
        "    def __init__(self, scale_factor, mode=\"nearest\"):\n",
        "        super(Upsample, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EmptyLayer(nn.Module):\n",
        "    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EmptyLayer, self).__init__()\n",
        "\n",
        "\n",
        "class YOLOLayer(nn.Module):\n",
        "    \"\"\"Detection layer\"\"\"\n",
        "\n",
        "    def __init__(self, anchors, num_classes, img_dim=1248):\n",
        "        super(YOLOLayer, self).__init__()\n",
        "        self.anchors = anchors\n",
        "        self.num_anchors = len(anchors)\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_thres = 0.5\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "        self.obj_scale = 1\n",
        "        self.noobj_scale = 100\n",
        "        self.metrics = {}\n",
        "        self.img_dim = img_dim\n",
        "        self.grid_size = 0  # grid size\n",
        "\n",
        "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
        "        self.grid_size = grid_size\n",
        "        g = self.grid_size\n",
        "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "        self.stride = self.img_dim / self.grid_size\n",
        "        # Calculate offsets for each grid\n",
        "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
        "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
        "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_l / self.stride, a_h ) for a_w, a_l, a_h in self.anchors])\n",
        "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
        "        self.anchor_l = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
        "        self.anchor_h = self.scaled_anchors[:, 2:3].view((1, self.num_anchors, 1, 1))\n",
        "\n",
        "    def forward(self, x, targets=None, img_dim=None):\n",
        "\n",
        "        # Tensors for cuda support\n",
        "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
        "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
        "        ByteTensor = torch.cuda.BoolTensor if x.is_cuda else torch.BoolTensor\n",
        "\n",
        "        self.img_dim = img_dim\n",
        "        num_samples = x.size(0)\n",
        "        grid_size = x.size(2)\n",
        "        prediction = (\n",
        "            x.view(num_samples, self.num_anchors, self.num_classes + 9, grid_size, grid_size)\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "            .contiguous()\n",
        "        )\n",
        "\n",
        "        # Get outputs\n",
        "        x = torch.sigmoid(prediction[..., 0])  # Center x\n",
        "        y = torch.sigmoid(prediction[..., 1])  # Center y\n",
        "        z = torch.sigmoid(prediction[..., 2])  # Center z\n",
        "        l = prediction[..., 3]  # Length\n",
        "        w = prediction[..., 4]  # Width\n",
        "        h = prediction[..., 5]  # Height\n",
        "        r1 = torch.sigmoid(prediction[..., 6])  # Rotation r1\n",
        "        r2 = torch.sigmoid(prediction[..., 7])  # Rotation r2\n",
        "        pred_conf = torch.sigmoid(prediction[..., 8])  # Conf\n",
        "        pred_cls = torch.sigmoid(prediction[..., 9:])  # Cls pred.\n",
        "        \n",
        "        # If grid size does not match current we compute new offsets\n",
        "        if grid_size != self.grid_size:\n",
        "            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n",
        "        # Add offset and scale with anchors\n",
        "        pred_boxes = FloatTensor(prediction[..., :8].shape)\n",
        "        pred_boxes[..., 0] = x.data + self.grid_x\n",
        "        pred_boxes[..., 1] = y.data + self.grid_y\n",
        "        pred_boxes[..., 2] = z.data\n",
        "        pred_boxes[..., 3] = torch.exp(w.data) * self.anchor_w\n",
        "        pred_boxes[..., 4] = torch.exp(l.data) * self.anchor_l\n",
        "        pred_boxes[..., 5] = torch.exp(h.data) * self.anchor_h\n",
        "        pred_boxes[..., 6] = r1.data\n",
        "        pred_boxes[..., 7] = r2.data\n",
        "\n",
        "        replacement = pred_boxes.view(num_samples, -1, 8)\n",
        "        replacement[...,:2] = replacement[...,:2] * self.stride\n",
        "        replacement[...,2] = replacement[...,2] * 5\n",
        "        replacement[...,3:5] = replacement[...,3:5] * self.stride\n",
        "        replacement[...,5] = replacement[...,5]\n",
        "        replacement[...,6] = replacement[...,6]\n",
        "        replacement[...,7] = replacement[...,7]\n",
        "\n",
        "\n",
        "        output = torch.cat(\n",
        "            (\n",
        "                replacement,\n",
        "                pred_conf.view(num_samples, -1, 1),\n",
        "                pred_cls.view(num_samples, -1, self.num_classes),\n",
        "            ),\n",
        "            -1,\n",
        "        )\n",
        "\n",
        "        if targets is None:\n",
        "            return output, 0\n",
        "        else:\n",
        "            class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf = build_targets(\n",
        "                pred_boxes=pred_boxes,\n",
        "                pred_cls=pred_cls,\n",
        "                target=targets,\n",
        "                anchors=self.scaled_anchors,\n",
        "                ignore_thres=self.ignore_thres,\n",
        "            )\n",
        "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
        "            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
        "            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
        "            loss_z = self.mse_loss(z[obj_mask], tz[obj_mask])\n",
        "            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
        "            loss_l = self.mse_loss(l[obj_mask], tl[obj_mask])\n",
        "            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
        "            loss_r1 = self.mse_loss(r1[obj_mask], tr1[obj_mask])\n",
        "            loss_r2 = self.mse_loss(r2[obj_mask], tr2[obj_mask])\n",
        "            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
        "            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
        "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
        "            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
        "            total_loss = loss_x + loss_y + loss_z + loss_w + loss_l + loss_h + loss_r1 + loss_r2 + loss_conf + loss_cls\n",
        "\n",
        "\n",
        "            return output, total_loss\n",
        "\n",
        "\n",
        "class Darknet(nn.Module):\n",
        "    \"\"\"YOLOv3 object detection model\"\"\"\n",
        "\n",
        "    def __init__(self, config_path, img_size=1248):\n",
        "        super(Darknet, self).__init__()\n",
        "        self.module_defs = parse_model_config(config_path)\n",
        "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
        "        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], \"metrics\")]\n",
        "        self.img_size = img_size\n",
        "        self.seen = 0\n",
        "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        img_dim = x.shape[2]\n",
        "        loss = 0\n",
        "        layer_outputs, yolo_outputs = [], []\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
        "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
        "                x = module(x)\n",
        "            elif module_def[\"type\"] == \"route\":\n",
        "                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
        "            elif module_def[\"type\"] == \"shortcut\":\n",
        "                layer_i = int(module_def[\"from\"])\n",
        "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
        "            elif module_def[\"type\"] == \"yolo\":\n",
        "                x, layer_loss = module[0](x, targets, img_dim)\n",
        "                loss += layer_loss\n",
        "                yolo_outputs.append(x)\n",
        "            layer_outputs.append(x)\n",
        "        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n",
        "        return yolo_outputs if targets is None else (loss, yolo_outputs)\n",
        "\n",
        "    def load_darknet_weights(self, weights_path):\n",
        "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
        "\n",
        "        # Open the weights file\n",
        "        with open(weights_path, \"rb\") as f:\n",
        "            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values\n",
        "            self.header_info = header  # Needed to write header when saving weights\n",
        "            self.seen = header[3]  # number of images seen during training\n",
        "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
        "\n",
        "        # Establish cutoff for loading backbone weights\n",
        "        cutoff = None\n",
        "        if \"darknet53.conv.74\" in weights_path:\n",
        "            cutoff = 75\n",
        "\n",
        "        ptr = 0\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
        "            if i == cutoff:\n",
        "                break\n",
        "            if module_def[\"type\"] == \"convolutional\":\n",
        "                conv_layer = module[0]\n",
        "                if module_def[\"batch_normalize\"]:\n",
        "                    # Load BN bias, weights, running mean and running variance\n",
        "                    bn_layer = module[1]\n",
        "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
        "                    # Bias\n",
        "                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n",
        "                    bn_layer.bias.data.copy_(bn_b)\n",
        "                    ptr += num_b\n",
        "                    # Weight\n",
        "                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n",
        "                    bn_layer.weight.data.copy_(bn_w)\n",
        "                    ptr += num_b\n",
        "                    # Running Mean\n",
        "                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n",
        "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
        "                    ptr += num_b\n",
        "                    # Running Var\n",
        "                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n",
        "                    bn_layer.running_var.data.copy_(bn_rv)\n",
        "                    ptr += num_b\n",
        "                else:\n",
        "                    # Load conv. bias\n",
        "                    num_b = conv_layer.bias.numel()\n",
        "                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n",
        "                    conv_layer.bias.data.copy_(conv_b)\n",
        "                    ptr += num_b\n",
        "                # Load conv. weights\n",
        "                num_w = conv_layer.weight.numel()\n",
        "                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n",
        "                conv_layer.weight.data.copy_(conv_w)\n",
        "                ptr += num_w\n",
        "\n",
        "    def save_darknet_weights(self, path, cutoff=-1):\n",
        "        \"\"\"\n",
        "            @:param path    - path of the new weights file\n",
        "            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
        "        \"\"\"\n",
        "        fp = open(path, \"wb\")\n",
        "        self.header_info[3] = self.seen\n",
        "        self.header_info.tofile(fp)\n",
        "\n",
        "        # Iterate through layers\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
        "            if module_def[\"type\"] == \"convolutional\":\n",
        "                conv_layer = module[0]\n",
        "                # If batch norm, load bn first\n",
        "                if module_def[\"batch_normalize\"]:\n",
        "                    bn_layer = module[1]\n",
        "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
        "                # Load conv bias\n",
        "                else:\n",
        "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                # Load conv weights\n",
        "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "\n",
        "        fp.close()\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t51RsFcXEMHk"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUGbtD3-FLvV"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def evaluate(model, data_samples, iou_thres, conf_thres, nms_thres, img_size, batch_size):\n",
        "    model.eval()\n",
        "\n",
        "    # Get dataloader\n",
        "    dataset = ListDataset(data_samples, img_size=img_size, augment=False, multiscale=False)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "    labels = []\n",
        "    sample_metrics = []  # List of tuples (TP, confs, pred)\n",
        "    for batch_i, (imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=\"Detecting objects\")):\n",
        "\n",
        "        # Extract labels\n",
        "        labels += targets[:, 1].tolist()\n",
        "        # Rescale target\n",
        "        targets[:, 2:8] *= img_size\n",
        "\n",
        "        imgs = Variable(imgs.type(Tensor), requires_grad=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(imgs)\n",
        "            outputs = non_max_suppression(outputs, conf_thres=conf_thres, nms_thres=nms_thres)\n",
        "\n",
        "        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)\n",
        "        #print(sample_metrics)\n",
        "        del imgs,targets,outputs\n",
        "        torch.cuda.empty_cache()\n",
        "    # Concatenate sample statistics\n",
        "    if not sample_metrics :\n",
        "      precision, recall, AP, f1, ap_class = np.asarray([0]),np.asarray([0]),np.asarray([0]),np.asarray([0]),[]\n",
        "    else :\n",
        "      true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
        "      precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
        "\n",
        "    return precision, recall, AP, f1, ap_class"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-CRslW0Xujc"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0zm8fD5ISjg"
      },
      "source": [
        "class parameters:\n",
        "  def __init__(self, pretrained_weights = \"weights/darknet53.conv.74\", epochs = 100, batch_size = 8, step_size = 4, model_def = \"config/yolov3custom.cfg\", n_cpu = 8, img_size = 416, checkpoint_interval = 1, evaluation_interval = 5, compute_map = False, multiscale_training = True):\n",
        "    self.epochs = epochs #number of epochs\n",
        "    self.batch_size = batch_size #size of each image batch\n",
        "    self.gradient_accumulations =step_size    #number of gradient accums before step\n",
        "    self.model_def = model_def #path to model definition file\n",
        "    #self.data_config = data_config #path to data config file`\n",
        "    self.pretrained_weights = pretrained_weights #if specified starts from checkpoint model\n",
        "    self.n_cpu = n_cpu #number of cpu threads to use during batch generation\n",
        "    self.img_size = img_size #size of each image dimension\n",
        "    self.checkpoint_interval = checkpoint_interval #interval between saving model weights\n",
        "    self.evaluation_interval = evaluation_interval #interval evaluations on validation set\n",
        "    self.compute_map = compute_map #if True computes mAP every tenth batch\n",
        "    self.multiscale_training = multiscale_training #allow for multi-scale training\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oI0p5QnYyK2"
      },
      "source": [
        "opt = parameters(epochs = 3, pretrained_weights = \"checkpoints/aws-short20-4.pth\",img_size = 1248,batch_size = 2,evaluation_interval = 1,checkpoint_interval=50,step_size=8)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPTzU1mCavSJ",
        "outputId": "2223c183-b253-419d-8c36-9c4cb70a9f33"
      },
      "source": [
        "model = Darknet(opt.model_def).to(device)\n",
        "model.apply(weights_init_normal) #applying initial weights to the model. Since it is using module apply recursively goes through each layer and initializes the specified weights weights_init_normal is a function in utils\n",
        "# If specified we start from checkpoint\n",
        "if opt.pretrained_weights:\n",
        "  if opt.pretrained_weights.endswith(\".pth\"):\n",
        "    model.load_state_dict(torch.load(opt.pretrained_weights))\n",
        "    print(\"Model LOADING Finished !!!\")\n",
        "  else:\n",
        "    model.load_darknet_weights(opt.pretrained_weights) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model LOADING Finished !!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIGSAkmDK3af",
        "outputId": "2a5e8219-9e81-48d5-b48d-ee5bfd6196d3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sample_mapping = []\n",
        "file_path = 'data/'\n",
        "for scene  in nusc.scene :\n",
        "  sample_token = scene['first_sample_token']\n",
        "  sample = nusc.get('sample',sample_token)\n",
        "  while not sample['next'] =='':\n",
        "    new_token = sample['token']\n",
        "    sample_mapping.append(new_token)\n",
        "    sample = nusc.get('sample',sample['next'])\n",
        "\n",
        "train_samples, test_samples = train_test_split(sample_mapping, test_size=0.2, random_state=42)\n",
        "print(len(train_samples),len(test_samples))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26639 6660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tAWjHQVsV9D"
      },
      "source": [
        "dataset = ListDataset(train_samples,  augment=False, multiscale=opt.multiscale_training)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    shuffle = True,\n",
        "    batch_size=opt.batch_size,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=dataset.collate_fn,\n",
        ")\n",
        "class_names = dataset.categories"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLdpDzAWZM0D"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr =0.0005)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "580e731f611a44288e817277f61d03f3",
            "61fc2b124152430e96a8c2eac6f084b4",
            "fbd1002ddf404d53992e18c827f8309a",
            "49ef9f6b17bd4b24900f37234e7951c6",
            "9c362cf9d428497e8cf10c3a85558ac2",
            "c1e418eca9d248a890f465f195249b99",
            "cc8a038214414e4c8f6a82c8679912fe",
            "101edabc10504730b5481307258ca473"
          ]
        },
        "id": "4tGdtKinQ-ll",
        "outputId": "4514967a-6abb-40b4-a000-381dd6a251b9"
      },
      "source": [
        "import tqdm\n",
        "import json\n",
        "\n",
        "#\n",
        "if os.path.exists(\"checkpoints/step-short20.json\"):\n",
        "  with open('checkpoints/step-short20.json') as f:\n",
        "    data = json.load(f)\n",
        "  step = data['step']\n",
        "else:\n",
        "  step = 0\n",
        "for epoch in tqdm.notebook.tqdm(range(opt.epochs),unit='epoch'):\n",
        "    model.train()\n",
        "    for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "        batches_done = len(dataloader) * epoch + batch_i\n",
        "        \n",
        "        imgs = Variable(imgs.to(device))\n",
        "        targets = Variable(targets.to(device), requires_grad=False)\n",
        "\n",
        "        loss, outputs = model(imgs, targets)\n",
        "        loss.backward()\n",
        "        loss = loss.cpu().detach().numpy()\n",
        "\n",
        "        try:\n",
        "          print(\"Step :\",step,\" -- \",batch_i,\"Batch\",loss,\"Targets = \",targets.shape[0],\"Ratio =\",loss/targets.shape[0],\"Threshold (0.5):\")#, len(non_max_suppression(outputs,conf_thres = 0.5,nms_thres=0.5)[0]) ,\",\", len(non_max_suppression(outputs,conf_thres = 0.5,nms_thres=0.5)[1]))\n",
        "        except:\n",
        "          print(\"Step :\",step,\" -- \",batch_i,\"Batch\",loss,\"Targets = \",targets.shape[0],\"Ratio =\",loss/targets.shape[0])\n",
        "          \n",
        "        if batches_done % opt.gradient_accumulations == 0 and batches_done != 0:\n",
        "            # Accumulates gradient before each step\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            print(\"->\",optimizer.param_groups[0]['lr'])\n",
        "            step += 1\n",
        "            steps ={'step':step}\n",
        "            if step == 400000:\n",
        "              optimizer.param_groups[0]['lr'] = 0.0001\n",
        "            elif step ==  450000 :\n",
        "              optimizer.param_groups[0]['lr'] = 0.00001\n",
        "\n",
        "        \n",
        "        if batch_i % opt.checkpoint_interval == 0 and batch_i != 0:\n",
        "            torch.save(model.state_dict(), \"checkpoints/aws-short20-4.pth\")\n",
        "            with open(\"checkpoints/step-short20.json\",'w+') as f:\n",
        "              json.dump(steps,f)\n",
        "            print(\"Saved\")\n",
        "            dpi = 80\n",
        "            im_data = imgs[0].cpu().permute(1, 2, 0)\n",
        "            height, width, depth = im_data.size()\n",
        "\n",
        "            # What size does the figure need to be in inches to fit the image?\n",
        "            figsize = width / float(dpi), height / float(dpi)\n",
        "\n",
        "            # Create a figure of the right size with one axes that takes up the full figure\n",
        "            fig = plt.figure(figsize=figsize)\n",
        "            ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "            # Hide spines, ticks, etc.\n",
        "            ax.axis('off')\n",
        "\n",
        "            # Display the image.\n",
        "            ax.imshow(im_data, cmap='gray')\n",
        "\n",
        "            plt.show()\n",
        "            try:\n",
        "                out = non_max_suppression(outputs,conf_thres = 0.7,nms_thres=0.08)\n",
        "                out[0][:,:2] /=1024\n",
        "                out[0][:,3:5] /=1024\n",
        "                draw(out[0])\n",
        "                print(\"threshold :\",0.7)\n",
        "            except:\n",
        "                try:\n",
        "                    out = non_max_suppression(outputs,conf_thres = 0.6,nms_thres=0.05)\n",
        "                    out[0][:,:2] /=1024\n",
        "                    out[0][:,3:5] /=1024\n",
        "                    draw(out[0])\n",
        "                    print(\"threshold :\",0.4)\n",
        "                except:\n",
        "                  print(\"Nothing !!!!\")\n",
        "\n",
        "        del imgs,targets,outputs,loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        \n",
        "    if epoch % opt.evaluation_interval == 0 :#and epoch != 0:\n",
        "        print(\"\\n---- Evaluating Model ----\")\n",
        "        # Evaluate the model on the validation set\n",
        "        precision, recall, AP, f1, ap_class = evaluate(\n",
        "            model,\n",
        "            test_samples,\n",
        "            iou_thres=0.5,\n",
        "            conf_thres=0.5,\n",
        "            nms_thres=0.02,\n",
        "            img_size=opt.img_size,\n",
        "            batch_size=2,\n",
        "        )\n",
        "        evaluation_metrics = [\n",
        "            (\"val_precision\", precision.mean()),\n",
        "            (\"val_recall\", recall.mean()),\n",
        "            (\"val_mAP\", AP.mean()),\n",
        "            (\"val_f1\", f1.mean()),\n",
        "        ]\n",
        "        #logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
        "\n",
        "        # Print class APs and mAP\n",
        "        ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "        for i, c in enumerate(ap_class):\n",
        "            ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "        print(AsciiTable(ap_table).table)\n",
        "        print(f\"---- mAP {AP.mean()}\")\n",
        "        with open(\"checkpoints/mAP.txt\",'a') as f:\n",
        "            f.write(AsciiTable(ap_table).table)\n",
        "            f.write('\\nmAP : {map}\\n'.format(map = AP.mean()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "580e731f611a44288e817277f61d03f3"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Step : 17337  --  0 Batch 8.794973 Targets =  20 Ratio = 0.4397486686706543 Threshold (0.5):\n",
            "Step : 17337  --  1 Batch 6.562742 Targets =  7 Ratio = 0.9375346047537667 Threshold (0.5):\n",
            "Step : 17337  --  2 Batch 6.2650924 Targets =  15 Ratio = 0.41767282485961915 Threshold (0.5):\n",
            "Step : 17337  --  3 Batch 5.6166363 Targets =  18 Ratio = 0.31203534868028426 Threshold (0.5):\n",
            "Step : 17337  --  4 Batch 6.9042454 Targets =  7 Ratio = 0.9863207680838448 Threshold (0.5):\n",
            "Step : 17337  --  5 Batch 10.792432 Targets =  43 Ratio = 0.25098678677581077 Threshold (0.5):\n",
            "Step : 17337  --  6 Batch 5.1962523 Targets =  17 Ratio = 0.3056619027081658 Threshold (0.5):\n",
            "Step : 17337  --  7 Batch 6.4039173 Targets =  13 Ratio = 0.49260902404785156 Threshold (0.5):\n",
            "Step : 17337  --  8 Batch 2.4069514 Targets =  8 Ratio = 0.3008689284324646 Threshold (0.5):\n",
            "-> 0.0005\n",
            "Step : 17338  --  9 Batch 9.801208 Targets =  16 Ratio = 0.6125754714012146 Threshold (0.5):\n",
            "Step : 17338  --  10 Batch 9.66667 Targets =  27 Ratio = 0.35802480909559464 Threshold (0.5):\n",
            "Step : 17338  --  11 Batch 8.692717 Targets =  9 Ratio = 0.9658573998345269 Threshold (0.5):\n",
            "Step : 17338  --  12 Batch 9.333787 Targets =  23 Ratio = 0.405816824539848 Threshold (0.5):\n",
            "Step : 17338  --  13 Batch 4.042796 Targets =  12 Ratio = 0.3368996779123942 Threshold (0.5):\n",
            "Step : 17338  --  14 Batch 11.55898 Targets =  28 Ratio = 0.412820713860648 Threshold (0.5):\n",
            "Step : 17338  --  15 Batch 8.255307 Targets =  11 Ratio = 0.7504824725064364 Threshold (0.5):\n",
            "Step : 17338  --  16 Batch 11.174607 Targets =  26 Ratio = 0.4297925875737117 Threshold (0.5):\n",
            "-> 0.0005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0DB9iZRbY2y"
      },
      "source": [
        "with open(\"checkpoints/mAP.txt\",'a') as f:\n",
        "  f.write(AsciiTable(ap_table).table)\n",
        "  f.write('\\nmAP : {map}'.format(map = AP.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2_WZsuGhnL_",
        "outputId": "7b2620f5-9dbb-499e-eef2-27876e45f8ca"
      },
      "source": [
        "!cat checkpoints/mAP.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mAP : 0.00019884741225546839\r\n",
            "mAP : 3.461213096768677e-05\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| Index | Class name                           | AP      |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| 0     | human.pedestrian.adult               | 0.00001 |\r\n",
            "| 1     | human.pedestrian.child               | 0.00000 |\r\n",
            "| 2     | human.pedestrian.wheelchair          | 0.00000 |\r\n",
            "| 3     | human.pedestrian.stroller            | 0.00000 |\r\n",
            "| 4     | human.pedestrian.personal_mobility   | 0.00000 |\r\n",
            "| 5     | human.pedestrian.police_officer      | 0.00000 |\r\n",
            "| 6     | human.pedestrian.construction_worker | 0.00000 |\r\n",
            "| 7     | animal                               | 0.00000 |\r\n",
            "| 8     | vehicle.car                          | 0.00017 |\r\n",
            "| 9     | vehicle.motorcycle                   | 0.00004 |\r\n",
            "| 10    | vehicle.bicycle                      | 0.00000 |\r\n",
            "| 11    | vehicle.bus.bendy                    | 0.00000 |\r\n",
            "| 12    | vehicle.bus.rigid                    | 0.00000 |\r\n",
            "| 13    | vehicle.truck                        | 0.00103 |\r\n",
            "| 14    | vehicle.construction                 | 0.00000 |\r\n",
            "| 16    | vehicle.emergency.police             | 0.00000 |\r\n",
            "| 17    | vehicle.trailer                      | 0.00000 |\r\n",
            "| 18    | movable_object.barrier               | 0.00000 |\r\n",
            "| 19    | movable_object.trafficcone           | 0.00000 |\r\n",
            "| 20    | movable_object.pushable_pullable     | 0.00000 |\r\n",
            "| 21    | movable_object.debris                | 0.00000 |\r\n",
            "| 22    | static_object.bicycle_rack           | 0.00000 |\r\n",
            "+-------+--------------------------------------+---------+mAP : 5.702298929895424e-05\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| Index | Class name                           | AP      |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| 0     | human.pedestrian.adult               | 0.00000 |\r\n",
            "| 1     | human.pedestrian.child               | 0.00000 |\r\n",
            "| 2     | human.pedestrian.wheelchair          | 0.00000 |\r\n",
            "| 3     | human.pedestrian.stroller            | 0.00000 |\r\n",
            "| 4     | human.pedestrian.personal_mobility   | 0.00000 |\r\n",
            "| 5     | human.pedestrian.police_officer      | 0.00000 |\r\n",
            "| 6     | human.pedestrian.construction_worker | 0.00000 |\r\n",
            "| 7     | animal                               | 0.00000 |\r\n",
            "| 8     | vehicle.car                          | 0.00009 |\r\n",
            "| 9     | vehicle.motorcycle                   | 0.00000 |\r\n",
            "| 10    | vehicle.bicycle                      | 0.00000 |\r\n",
            "| 11    | vehicle.bus.bendy                    | 0.00000 |\r\n",
            "| 12    | vehicle.bus.rigid                    | 0.00000 |\r\n",
            "| 13    | vehicle.truck                        | 0.00024 |\r\n",
            "| 14    | vehicle.construction                 | 0.00000 |\r\n",
            "| 16    | vehicle.emergency.police             | 0.00000 |\r\n",
            "| 17    | vehicle.trailer                      | 0.00315 |\r\n",
            "| 18    | movable_object.barrier               | 0.00000 |\r\n",
            "| 19    | movable_object.trafficcone           | 0.00000 |\r\n",
            "| 20    | movable_object.pushable_pullable     | 0.00000 |\r\n",
            "| 21    | movable_object.debris                | 0.00000 |\r\n",
            "| 22    | static_object.bicycle_rack           | 0.00000 |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "mAP : 0.00015868305394661262\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| Index | Class name                           | AP      |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| 0     | human.pedestrian.adult               | 0.00847 |\r\n",
            "| 1     | human.pedestrian.child               | 0.00000 |\r\n",
            "| 2     | human.pedestrian.wheelchair          | 0.00000 |\r\n",
            "| 3     | human.pedestrian.stroller            | 0.00000 |\r\n",
            "| 4     | human.pedestrian.personal_mobility   | 0.00000 |\r\n",
            "| 5     | human.pedestrian.police_officer      | 0.00000 |\r\n",
            "| 6     | human.pedestrian.construction_worker | 0.00000 |\r\n",
            "| 7     | animal                               | 0.00000 |\r\n",
            "| 8     | vehicle.car                          | 0.30474 |\r\n",
            "| 9     | vehicle.motorcycle                   | 0.05202 |\r\n",
            "| 10    | vehicle.bicycle                      | 0.00742 |\r\n",
            "| 11    | vehicle.bus.bendy                    | 0.00000 |\r\n",
            "| 12    | vehicle.bus.rigid                    | 0.00972 |\r\n",
            "| 13    | vehicle.truck                        | 0.09200 |\r\n",
            "| 14    | vehicle.construction                 | 0.03427 |\r\n",
            "| 16    | vehicle.emergency.police             | 0.00000 |\r\n",
            "| 17    | vehicle.trailer                      | 0.01856 |\r\n",
            "| 18    | movable_object.barrier               | 0.00036 |\r\n",
            "| 19    | movable_object.trafficcone           | 0.00566 |\r\n",
            "| 20    | movable_object.pushable_pullable     | 0.00154 |\r\n",
            "| 21    | movable_object.debris                | 0.00000 |\r\n",
            "| 22    | static_object.bicycle_rack           | 0.03604 |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "mAP : 0.02594514751841035"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc-2Uf6MlQiE",
        "outputId": "8a0d0f3d-fe99-4cd8-9c21-052c54e1b2d1"
      },
      "source": [
        "precision, recall, AP, f1, ap_class = evaluate(\n",
        "    model,\n",
        "    test_samples,\n",
        "    iou_thres=0.5,\n",
        "    conf_thres=0.7,\n",
        "    nms_thres=0.07, \n",
        "    img_size=opt.img_size,\n",
        "    batch_size=2,\n",
        ")\n",
        "evaluation_metrics = [\n",
        "    (\"val_precision\", precision.mean()),\n",
        "    (\"val_recall\", recall.mean()),\n",
        "    (\"val_mAP\", AP.mean()),\n",
        "    (\"val_f1\", f1.mean()),\n",
        "]\n",
        "#logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
        "\n",
        "# Print class APs and mAP\n",
        "ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "for i, c in enumerate(ap_class):\n",
        "    ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "print(AsciiTable(ap_table).table)\n",
        "print(f\"---- mAP {AP.mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3330/3330 [1:51:23<00:00,  2.01s/it]\n",
            "Computing AP: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 130.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------------------------+---------+\n",
            "| Index | Class name                           | AP      |\n",
            "+-------+--------------------------------------+---------+\n",
            "| 0     | human.pedestrian.adult               | 0.00847 |\n",
            "| 1     | human.pedestrian.child               | 0.00000 |\n",
            "| 2     | human.pedestrian.wheelchair          | 0.00000 |\n",
            "| 3     | human.pedestrian.stroller            | 0.00000 |\n",
            "| 4     | human.pedestrian.personal_mobility   | 0.00000 |\n",
            "| 5     | human.pedestrian.police_officer      | 0.00000 |\n",
            "| 6     | human.pedestrian.construction_worker | 0.00000 |\n",
            "| 7     | animal                               | 0.00000 |\n",
            "| 8     | vehicle.car                          | 0.30474 |\n",
            "| 9     | vehicle.motorcycle                   | 0.05202 |\n",
            "| 10    | vehicle.bicycle                      | 0.00742 |\n",
            "| 11    | vehicle.bus.bendy                    | 0.00000 |\n",
            "| 12    | vehicle.bus.rigid                    | 0.00972 |\n",
            "| 13    | vehicle.truck                        | 0.09200 |\n",
            "| 14    | vehicle.construction                 | 0.03427 |\n",
            "| 16    | vehicle.emergency.police             | 0.00000 |\n",
            "| 17    | vehicle.trailer                      | 0.01856 |\n",
            "| 18    | movable_object.barrier               | 0.00036 |\n",
            "| 19    | movable_object.trafficcone           | 0.00566 |\n",
            "| 20    | movable_object.pushable_pullable     | 0.00154 |\n",
            "| 21    | movable_object.debris                | 0.00000 |\n",
            "| 22    | static_object.bicycle_rack           | 0.03604 |\n",
            "+-------+--------------------------------------+---------+\n",
            "---- mAP 0.02594514751841035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eNn34W70xfj"
      },
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "  print(param_group['lr'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWea0HlsTB1a"
      },
      "source": [
        "img, tragets = next(iter(dataloader))\n",
        "img = Variable(img.to(device))\n",
        "model.eval()\n",
        "target = []\n",
        "for t in tragets:\n",
        "  if t[0] == 0:\n",
        "    target.append(t[2:])\n",
        "output = model(img)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "kgRJieLM2qob",
        "outputId": "275473a4-20b2-4a29-a0d4-7c5c05d81da2"
      },
      "source": [
        "out = non_max_suppression(output,conf_thres = 0.75,nms_thres = 0.1)\n",
        "out[0][:,:2] /=1024\n",
        "out[0][:,3:5] /=1024\n",
        "#print(loss)\n",
        "draw(target)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "5jHwWqMFX8n5",
        "outputId": "8b92f1df-ee8a-4938-c409-24531d3270a9"
      },
      "source": [
        "draw(out[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "d2qIApnwh5pK",
        "outputId": "85b4d0e4-a79c-4b00-f0cf-2116b7fef96f"
      },
      "source": [
        "draw(out[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shkMUZFghdKy"
      },
      "source": [
        "dpi = 80\n",
        "im_data = img[0].cpu().permute(1, 2, 0)\n",
        "height, width, depth = im_data.shape\n",
        "\n",
        "# What size does the figure need to be in inches to fit the image?\n",
        "figsize = width / float(dpi), height / float(dpi)\n",
        "\n",
        "# Create a figure of the right size with one axes that takes up the full figure\n",
        "fig = plt.figure(figsize=figsize)\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "# Hide spines, ticks, etc.\n",
        "ax.axis('off')\n",
        "\n",
        "# Display the image.\n",
        "ax.imshow(im_data, cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M1c8A9sm6ny"
      },
      "source": [
        "!mv \"singledepthyolo(leastnoparameters-starting).pth\" checkpoints/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jfQRuyqhHq"
      },
      "source": [
        "#Testing (Nuscene's Basics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1v635cuk858"
      },
      "source": [
        "classes = nusc.get('category',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVYn959sH3Pz"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxTzVdt1HyNt"
      },
      "source": [
        "img, label = next(iter(dataloader))\n",
        "print(img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q89sfq4sHyPY"
      },
      "source": [
        "loss, out = model(img.to(device),label.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EL2lkUEUaeZ"
      },
      "source": [
        "print(non_max_suppression(out.to(device))[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6K8n4CZT7Ft"
      },
      "source": [
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-DD17QtR2dq"
      },
      "source": [
        "del imgs,targets,out,loss\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgzPg8rc4Bse"
      },
      "source": [
        "print(label[:,2].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rju84T256zm-"
      },
      "source": [
        "print(label[:,2].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3TwxDkCqj-g"
      },
      "source": [
        "%matplotlib inline\n",
        "from nuscenes.nuscenes import NuScenes \n",
        "\n",
        "nusc = NuScenes(version='v1.0-mini', dataroot='drive/My Drive/data', verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A13iNdjFqkm_"
      },
      "source": [
        "scene = nusc.scene[0]\n",
        "scene"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkw-qStdrcmq"
      },
      "source": [
        "sample_token = scene['first_sample_token']\n",
        "sample = nusc.get('sample',sample_token)\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0WDSEtHOK0O"
      },
      "source": [
        "for first_anno in sample['anns'] :\n",
        "  anno = nusc.get('sample_annotation',first_anno)\n",
        "  vis = nusc.get('visibility',anno['visibility_token'])\n",
        "  print(vis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u035LGwgPRiA"
      },
      "source": [
        "vis = nusc.get('visibility',anno['visibility_token'])\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9b82vLGnJ4v"
      },
      "source": [
        "import json\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/data/v1.0-mini/category.json') as f:\n",
        "  data = json.load(f)\n",
        "for d in data:\n",
        "  print(d['name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SLzR8LenKIE"
      },
      "source": [
        "img,targets = iter(dataloader).next()\n",
        "plt.imshow(  img[0].permute(1, 2, 0)  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvAnhNUPhFgU"
      },
      "source": [
        "output = model(img.to(device))\n",
        "print(output[...,8].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX05wMiohJNz"
      },
      "source": [
        "output = non_max_suppression(output,conf_thres=0.9)\n",
        "print(output[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw4rYBEDWKaY"
      },
      "source": [
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU7biujtWx1R"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib as mpl\n",
        "import matplotlib.patches as patches\n",
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcQwss6bXKoR"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111)\n",
        "for annotation in targets:\n",
        "  annotation = nusc.get('sample_annotation',annotation)\n",
        "  cordinate = [annotation['translation'][i] - ego_pose['translation'][i] for i in range(3)]\n",
        "  cordinate[0], cordinate[1] = rotate_around_point_lowperf(cordinate[:2], ego_yaw, origin=(0, 0))\n",
        "  rotation_yaw = quaternion_yaw(annotation['rotation']) - math.pi/2\n",
        "  print(\"rotation_yaw = \", quaternion_yaw(annotation['rotation']) + math.pi )\n",
        "  object_rotation = rotation_yaw - ego_yaw\n",
        "  height = annotation['size'][0]\n",
        "  width = annotation['size'][1]\n",
        "  x_temp, y_temp = rotate_around_point_lowperf((cordinate[0],cordinate[1]),2*math.pi -  (object_rotation+math.pi/2), origin=(cordinate[0]-width/2, cordinate[1] - height/2))\n",
        "  x_offset, y_offset = x_temp - cordinate[0], y_temp - cordinate[1]\n",
        "  rectas = patches.Rectangle(xy=((cordinate[0]-width/2) - x_offset, (cordinate[1] - height/2) - y_offset) ,width=width, angle = (object_rotation+math.pi/2)*180/math.pi, height=height, linewidth=1, color='blue', fill=False)\n",
        "  ax.add_patch(rectas)\n",
        "  ax.scatter(cordinate[0], cordinate[1], color = 'red', s=10)\n",
        "  break\n",
        "ax.scatter(0, 0)\n",
        "plt.xlim(-80,80)\n",
        "plt.ylim(-80,80)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xsHPdyHjeJ7"
      },
      "source": [
        "dataset.categories[18]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwvIc7iFZ__d"
      },
      "source": [
        "#Angle testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbhkMNF-aCGL"
      },
      "source": [
        "#30degree\n",
        "rotation_yaw = 0\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT11qiCtaLLS"
      },
      "source": [
        "#120degree\n",
        "rotation_yaw = math.pi/2 \n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjJ5mE2TaN6H"
      },
      "source": [
        "#210degree\n",
        "rotation_yaw = math.pi\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXNmm2SQavVU"
      },
      "source": [
        "#300degree\n",
        "rotation_yaw = 3*math.pi/2\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TGwbnheayIx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_2iY7HwCTlm"
      },
      "source": [
        "#Intersection Area Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLZVErYZxX4D"
      },
      "source": [
        "from turfpy.transformation import intersect\n",
        "from turfpy.measurement import area\n",
        "from geojson import Feature\n",
        "f = Feature(geometry={\"coordinates\": [\n",
        "[[-122.801742, 45.48565], [-122.801742, 45.60491],\n",
        "[-122.584762, 45.60491], [-122.584762, 45.48565],\n",
        "[-122.801742, 45.48565]]], \"type\": \"Polygon\"})\n",
        "b = Feature(geometry={\"coordinates\": [\n",
        "[[-122.520217, 45.535693], [-122.64038, 45.553967],\n",
        "[-122.720031, 45.526554], [-122.669906, 45.507309],\n",
        "[-122.723464, 45.446643], [-122.532577, 45.408574],\n",
        "[-122.487258, 45.477466], [-122.520217, 45.535693]\n",
        "]], \"type\": \"Polygon\"})\n",
        "inter = intersect([f, b])\n",
        "area(inter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CePQxQ8Uxaye"
      },
      "source": [
        "!pip install turfpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HSDxXySxjqN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}