{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AWS_SingleImgDepth_(Yolov3)Notebookversion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1EU_yqgXM4XV",
        "I7qx7VfcNVgV",
        "aNulh2XvM748",
        "5crs6F9brpwq",
        "takpM5RvrupZ",
        "t51RsFcXEMHk",
        "m0jfQRuyqhHq",
        "IwvIc7iFZ__d",
        "8_2iY7HwCTlm"
      ]
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.17"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd15af9a685a4c268890765c47b979d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb492cf1a7894c788d8768c8f4cc0060",
              "IPY_MODEL_6f714e6fe57a4e0da5bc1fb5a340b171"
            ],
            "layout": "IPY_MODEL_6434d127fc2946f094813511710e3f2d"
          }
        },
        "cb492cf1a7894c788d8768c8f4cc0060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "  0%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ff5dbeed07463f98bad70e0ce3a39e",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9071d2727fbd4e94b0bdc7c339d6a972",
            "value": 0
          }
        },
        "6f714e6fe57a4e0da5bc1fb5a340b171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d6f27faa36a4d4eb18215a888777325",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6a518f40b2bb4d749a404dec540ed358",
            "value": " 0/3 [00:00&lt;?, ?epoch/s]"
          }
        },
        "6434d127fc2946f094813511710e3f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ff5dbeed07463f98bad70e0ce3a39e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9071d2727fbd4e94b0bdc7c339d6a972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2d6f27faa36a4d4eb18215a888777325": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a518f40b2bb4d749a404dec540ed358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EU_yqgXM4XV"
      },
      "source": [
        "#Anchor Box Calculation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2N5uR4jM32T"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class main:\n",
        "  def __init__(self,no):\n",
        "    self.num_clusters = no\n",
        "\n",
        "   \n",
        "args = main(9)\n",
        "\n",
        "'''\n",
        "Created on Feb 20, 2017\n",
        "@author: jumabek\n",
        "'''\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import argparse\n",
        "#import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import random \n",
        "import math\n",
        "\n",
        "width_in_cfg_file = 1248.\n",
        "height_in_cfg_file = 1248.\n",
        "\n",
        "def IOU(X,centroids):\n",
        "    similarities = []\n",
        "    k = len(centroids)\n",
        "    for centroid in centroids:\n",
        "        c_x,c_y,c_z = centroid\n",
        "        x,y,z = X\n",
        "        inner_vol = min(x,c_x)*min(y,c_y)*min(z,c_z)\n",
        "        similarity = inner_vol/((x*y*z+c_x*c_y*c_z)-inner_vol)\n",
        "        similarities.append(similarity) # will become (k,) shape\n",
        "    return np.array(similarities) \n",
        "\n",
        "def avg_IOU(X,centroids):\n",
        "    n,d = X.shape\n",
        "    sum = 0.\n",
        "    for i in range(X.shape[0]):\n",
        "        #note IOU() will return array which contains IoU for each centroid and X[i] // slightly ineffective, but I am too lazy\n",
        "        sum+= max(IOU(X[i],centroids)) \n",
        "    return sum/n\n",
        "\n",
        "def write_anchors_to_file(centroids,X,anchor_file):\n",
        "    f = open(anchor_file,'w')\n",
        "    \n",
        "    anchors = centroids.copy()\n",
        "    print(anchors.shape)\n",
        "\n",
        "    for i in range(anchors.shape[0]):\n",
        "        anchors[i][0]*=width_in_cfg_file\n",
        "        anchors[i][1]*=height_in_cfg_file\n",
        "        anchors[i][2]*= 5\n",
        "         \n",
        "\n",
        "    widths = anchors[:,0]+ anchors[:,1]\n",
        "    sorted_indices = np.argsort(widths)\n",
        "\n",
        "    print('Anchors = ', anchors[sorted_indices])\n",
        "        \n",
        "    for i in sorted_indices[:-1]:\n",
        "        f.write('%0.5f,%0.5f,%0.5f, '%(anchors[i,0],anchors[i,1],anchors[i,2]))\n",
        "\n",
        "    #there should not be comma after last anchor, that's why\n",
        "    f.write('%0.5f,%0.5f,%0.5f\\n'%(anchors[sorted_indices[-1:],0],anchors[sorted_indices[-1:],1],anchors[sorted_indices[-1:],2]))\n",
        "    \n",
        "    f.write('%f\\n'%(avg_IOU(X,centroids)))\n",
        "    plt.scatter(centroids.shape[0], avg_IOU(X,centroids))\n",
        "\n",
        "def kmeans(X,centroids,eps,anchor_file):\n",
        "    \n",
        "    N = X.shape[0]\n",
        "    iterations = 0\n",
        "    k,dim = centroids.shape\n",
        "    print(\"k,dim =\",k,dim)\n",
        "    prev_assignments = np.ones(N)*(-1)    \n",
        "    iter = 0\n",
        "    old_D = np.zeros((N,k))\n",
        "\n",
        "    while True:\n",
        "        D = [] \n",
        "        iter+=1           \n",
        "        for i in range(N):\n",
        "            d = 1 - IOU(X[i],centroids)\n",
        "            D.append(d)\n",
        "        D = np.array(D) # D.shape = (N,k)\n",
        "        \n",
        "        print(\"iter {}: dists = {}\".format(iter,np.sum(np.abs(old_D-D))))\n",
        "            \n",
        "        #assign samples to centroids \n",
        "        assignments = np.argmin(D,axis=1)\n",
        "        \n",
        "        if (assignments == prev_assignments).all() :\n",
        "            print(\"Centroids = \",centroids)\n",
        "            write_anchors_to_file(centroids,X,anchor_file)\n",
        "            return\n",
        "\n",
        "        #calculate new centroids\n",
        "        centroid_sums=np.zeros((k,dim),np.float)\n",
        "        for i in range(N):\n",
        "            centroid_sums[assignments[i]]+=X[i]        \n",
        "        for j in range(k):            \n",
        "            centroids[j] = centroid_sums[j]/(np.sum(assignments==j))\n",
        "        \n",
        "        prev_assignments = assignments.copy()     \n",
        "        old_D = D.copy()  \n",
        "\n",
        "    \n",
        "\n",
        "annotation_dims = []\n",
        "\n",
        "size = np.zeros((1,1,3))\n",
        "file_path = '/gdrive/My Drive/data/'\n",
        "for scene  in nusc.scene :\n",
        "  sample_token = scene['first_sample_token']\n",
        "  sample = nusc.get('sample',sample_token)\n",
        "  sensor = 'LIDAR_TOP'\n",
        "  lidar_top_data = nusc.get('sample_data', sample['data'][sensor])\n",
        "  ego_pose = nusc.get('ego_pose', lidar_top_data['ego_pose_token'])\n",
        "  for annotation in sample['anns']:\n",
        "    annotation = nusc.get('sample_annotation',annotation)\n",
        "    x,y,z = annotation['size']\n",
        "    x = float(x) / 140.\n",
        "    y = float(y) / 140.\n",
        "    z = float(z) / 5.\n",
        "    annotation_dims.append(tuple(map(float,(x,y,z))))\n",
        "#print(annotation_dims)  \n",
        "annotation_dims = np.array(annotation_dims)\n",
        "\n",
        "eps = 0.005\n",
        "\n",
        "if args.num_clusters == 0:\n",
        "    for num_clusters in range(1,11): #we make 1 through 10 clusters \n",
        "        anchor_file = 'anchors%d.txt'%(num_clusters)\n",
        "\n",
        "        indices = [ random.randrange(annotation_dims.shape[0]) for i in range(num_clusters)]\n",
        "        centroids = annotation_dims[indices]\n",
        "        kmeans(annotation_dims,centroids,eps,anchor_file)\n",
        "        print('centroids.shape', centroids.shape)\n",
        "else:\n",
        "    anchor_file = 'anchors%d.txt'%(args.num_clusters)\n",
        "    indices = [ random.randrange(annotation_dims.shape[0]) for i in range(args.num_clusters)]\n",
        "    centroids = annotation_dims[indices]\n",
        "    kmeans(annotation_dims,centroids,eps,anchor_file)\n",
        "    print('centroids.shape', centroids.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu1geigrSzaD"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "annotation_dims = []\n",
        "cat = []\n",
        "size = np.zeros((1,1,3))\n",
        "file_path = '/gdrive/My Drive/data/'\n",
        "for scene  in nusc.scene :\n",
        "  sample_token = scene['first_sample_token']\n",
        "  sample = nusc.get('sample',sample_token)\n",
        "  sensor = 'LIDAR_TOP'\n",
        "  lidar_top_data = nusc.get('sample_data', sample['data'][sensor])\n",
        "  ego_pose = nusc.get('ego_pose', lidar_top_data['ego_pose_token'])\n",
        "  for annotation in sample['anns']:\n",
        "    annotation = nusc.get('sample_annotation',annotation)\n",
        "    print(annotation['size'],annotation['category_name'])\n",
        "    x,y,z = annotation['size']\n",
        "    cat.append(annotation['category_name'])\n",
        "    #x = float(x) / 140.\n",
        "    #y = float(y) / 140.\n",
        "    #z = float(z) / 5.\n",
        "    annotation_dims.append(tuple(map(float,(x,y,z))))\n",
        "print(annotation_dims)\n",
        "d = Counter(cat)   \n",
        "print(d)\n",
        "print(annotation_dims[1])\n",
        "print(max(x[0] for x in annotation_dims))\n",
        "print(max(x[1] for x in annotation_dims))\n",
        "print(max(x[2] for x in annotation_dims))\n",
        "#print(sum(annotation_dims[:,1]))\n",
        "#print(sum(annotation_dims[:,2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vfkhkwxPhJa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5rdnxJVl4LZ"
      },
      "source": [
        "!pip3 install terminaltables\n",
        "!pip install nuscenes-devkit\n",
        "!pip install turfpy\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7qx7VfcNVgV"
      },
      "source": [
        "#data test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOKpxjoY8_8i"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "i = ListDataset(train_samples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMIOzarKLNXP"
      },
      "source": [
        "img, targets = i.__getitem__(2)\n",
        "dpi = 80\n",
        "print(img.shape)\n",
        "im_data = img.permute(1, 2, 0)\n",
        "height, width, depth = 1024, 1024, 3\n",
        "\n",
        "# What size does the figure need to be in inches to fit the image?\n",
        "figsize = width / float(dpi), height / float(dpi)\n",
        "\n",
        "# Create a figure of the right size with one axes that takes up the full figure\n",
        "fig = plt.figure(figsize=figsize)\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "# Hide spines, ticks, etc.\n",
        "ax.axis('off')\n",
        "\n",
        "# Display the image.\n",
        "ax.imshow(im_data, cmap='gray')\n",
        "\n",
        "plt.show()\n",
        "#print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LklsSzDNP6s8"
      },
      "source": [
        "img, targets = i.__getitem__(1)\n",
        "plt.imshow(  img.permute(1, 2, 0)  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNulh2XvM748"
      },
      "source": [
        "#Start off"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLnS-Foc5Zav",
        "outputId": "2b0c1e4f-bbb3-4cf3-aad6-ba7a7b03c0c8"
      },
      "source": [
        "%matplotlib inline\n",
        "from nuscenes.nuscenes import NuScenes \n",
        "\n",
        "nusc = NuScenes(version='v1.0-trainval', dataroot='data', verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======\n",
            "Loading NuScenes tables for version v1.0-trainval...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "64386 instance,\n",
            "12 sensor,\n",
            "10200 calibrated_sensor,\n",
            "2631083 ego_pose,\n",
            "68 log,\n",
            "850 scene,\n",
            "34149 sample,\n",
            "2631083 sample_data,\n",
            "1166187 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 43.172 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 9.3 seconds.\n",
            "======\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTQzGTHQ0FfF"
      },
      "source": [
        "#import wandb\n",
        "#wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH_zIRd2IP1j"
      },
      "source": [
        "\n",
        "from __future__ import division\n",
        "\n",
        "from terminaltables import AsciiTable\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5crs6F9brpwq"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q0mYRDmhNzp"
      },
      "source": [
        "#Angle Decoder\n",
        "\n",
        "def angle_decoder(r):\n",
        "  teta1 = torch.asin(2*r[0] - 1)\n",
        "  teta2 = torch.acos(2*r[1] - 1)\n",
        "  teta = 0\n",
        "  if 2*r[0] - 1 >= 0 and 2*r[1] - 1 >= 0:\n",
        "    teta = (teta1+teta2)/2\n",
        "  elif 2*r[0] - 1 >= 0 and 2*r[1] - 1 < 0:\n",
        "    teta = (math.pi-teta1+teta2)/2\n",
        "  elif 2*r[0] - 1 < 0 and 2*r[1] - 1 <= 0:\n",
        "    teta = (math.pi - teta1+2*math.pi -teta2)/2\n",
        "  elif 2*r[0] - 1 < 0 and 2*r[1] - 1 > 0:\n",
        "    teta = (2*math.pi + teta1+2*math.pi - teta2)/2\n",
        "  return teta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8QQyIXmraeD"
      },
      "source": [
        "#Utils\n",
        "\n",
        "from __future__ import division\n",
        "import math\n",
        "import time\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "\n",
        "def to_cpu(tensor):\n",
        "    return tensor.detach().cpu()\n",
        "\n",
        "def rotate_around_point(point, radians, origin=(0, 0)):\n",
        "  \"\"\"Rotate a point around a given point.\n",
        "  \n",
        "  I call this the \"low performance\" version since it's recalculating\n",
        "  the same values more than once [cos(radians), sin(radians), x-ox, y-oy).\n",
        "  It's more readable than the next function, though.\n",
        "  \"\"\"\n",
        "  x, y = point\n",
        "  ox, oy = origin\n",
        "\n",
        "  qx = ox + math.cos(radians) * (x - ox) + math.sin(radians) * (y - oy)\n",
        "  qy = oy + -math.sin(radians) * (x - ox) + math.cos(radians) * (y - oy)\n",
        "\n",
        "  return qx.item(), qy.item()\n",
        "\n",
        "def load_classes(path):\n",
        "    \"\"\"\n",
        "    Loads class labels at 'path'\n",
        "    \"\"\"\n",
        "    fp = open(path, \"r\")\n",
        "    names = fp.read().split(\"\\n\")[:-1]\n",
        "    return names\n",
        "\n",
        "\n",
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "def rescale_boxes(boxes, current_dim, original_shape):\n",
        "    \"\"\" Rescales bounding boxes to the original shape \"\"\"\n",
        "    orig_h, orig_w = original_shape\n",
        "    # The amount of padding that was added\n",
        "    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n",
        "    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n",
        "    # Image height and width after padding is removed\n",
        "    unpad_h = current_dim - pad_y\n",
        "    unpad_w = current_dim - pad_x\n",
        "    # Rescale bounding boxes to dimension of original image\n",
        "    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n",
        "    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n",
        "    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n",
        "    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
        "    # Arguments\n",
        "        tp:    True positives (list).\n",
        "        conf:  Objectness value from 0-1 (list).\n",
        "        pred_cls: Predicted object classes (list).\n",
        "        target_cls: True object classes (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort by objectness\n",
        "    i = np.argsort(-conf)\n",
        "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes = np.unique(target_cls)\n",
        "\n",
        "    # Create Precision-Recall curve and compute AP for each class\n",
        "    ap, p, r = [], [], []\n",
        "    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n",
        "        i = pred_cls == c\n",
        "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
        "        n_p = i.sum()  # Number of predicted objects\n",
        "\n",
        "        if n_p == 0 and n_gt == 0:\n",
        "            continue\n",
        "        elif n_p == 0 or n_gt == 0:\n",
        "            ap.append(0)\n",
        "            r.append(0)\n",
        "            p.append(0)\n",
        "        else:\n",
        "            # Accumulate FPs and TPs\n",
        "            fpc = (1 - tp[i]).cumsum()\n",
        "            tpc = (tp[i]).cumsum()\n",
        "\n",
        "            # Recall\n",
        "            recall_curve = tpc / (n_gt + 1e-16)\n",
        "            r.append(recall_curve[-1])\n",
        "\n",
        "            # Precision\n",
        "            precision_curve = tpc / (tpc + fpc)\n",
        "            p.append(precision_curve[-1])\n",
        "\n",
        "            # AP from recall-precision curve\n",
        "            ap.append(compute_ap(recall_curve, precision_curve))\n",
        "\n",
        "    # Compute F1 score (harmonic mean of precision and recall)\n",
        "    p, r, ap = np.array(p), np.array(r), np.array(ap)\n",
        "    f1 = 2 * p * r / (p + r + 1e-16)\n",
        "\n",
        "    return p, r, ap, f1, unique_classes.astype(\"int32\")\n",
        "\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
        "\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list).\n",
        "        precision: The precision curve (list).\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "    # correct AP calculation\n",
        "    # first append sentinel values at the end\n",
        "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
        "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
        "\n",
        "    # compute the precision envelope\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "    # to calculate area under PR curve, look for points\n",
        "    # where X axis (recall) changes value\n",
        "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "    # and sum (\\Delta recall) * prec\n",
        "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "    return ap\n",
        "\n",
        "\n",
        "def get_batch_statistics(outputs, targets, iou_threshold):\n",
        "    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n",
        "    batch_metrics = []\n",
        "    for sample_i in range(len(outputs)):\n",
        "\n",
        "        if outputs[sample_i] is None:\n",
        "            continue\n",
        "\n",
        "        output = outputs[sample_i]\n",
        "        pred_boxes = output[:, :8]\n",
        "        pred_scores = output[:, 8]\n",
        "        pred_labels = output[:, -1]\n",
        "\n",
        "        true_positives = np.zeros(pred_boxes.shape[0])\n",
        "\n",
        "        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n",
        "        target_labels = annotations[:, 0] if len(annotations) else []\n",
        "        if len(annotations):\n",
        "            detected_boxes = []\n",
        "            target_boxes = annotations[:, 1:]\n",
        "\n",
        "            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n",
        "\n",
        "                # If targets are found break\n",
        "                if len(detected_boxes) == len(annotations):\n",
        "                    break\n",
        "\n",
        "                # Ignore if label is not one of the target labels\n",
        "                if pred_label not in target_labels:\n",
        "                    continue\n",
        "\n",
        "                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n",
        "                if iou >= iou_threshold and box_index not in detected_boxes:\n",
        "                    true_positives[pred_i] = 1\n",
        "                    detected_boxes += [box_index]\n",
        "        batch_metrics.append([true_positives, pred_scores, pred_labels])\n",
        "    return batch_metrics\n",
        "\n",
        "\n",
        "def bbox_wh_iou(wh1, wh2):\n",
        "    wh2 = wh2.t()\n",
        "    w1, l1, h1 = wh1[0], wh1[1] ,wh1[2]\n",
        "    w2, l2, h2 = wh2[0], wh2[1] ,wh1[2]\n",
        "    inter_area = torch.min(w1, w2) * torch.min(l1, l2) * torch.min(h1, h2)\n",
        "    union_area = (w1 * l1 * h1 + 1e-16) + (w2 * l2 * h2) - inter_area\n",
        "    return inter_area / union_area\n",
        "\n",
        "\n",
        "\n",
        "def bbox_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Returns the IoU of two bounding boxes\n",
        "    \"\"\"\n",
        "    #print(\"box shapes\",box1.shape,box2.shape)\n",
        "    iou_scores = []\n",
        "    # Transform from center and width to exact coordinates\n",
        "    b1_x1, b1_x3 = box1[:,0] - box1[:,3] / 2, box1[:,0] + box1[:,3] / 2\n",
        "    b1_y1, b1_y3 = box1[:,1] - box1[:,4] / 2, box1[:,1] + box1[:,4] / 2\n",
        "    b1_x2, b1_x4 = b1_x3, b1_x1\n",
        "    b1_y2, b1_y4 = b1_y1, b1_y3\n",
        "    #b1_z1, b1_z3 = box1[2] - box1[5] / 2, box1[2] + box1[5] / 2\n",
        "    b2_x1, b2_x3 = box2[:,0] - box2[:,3] / 2, box2[:,0] + box2[:,3] / 2\n",
        "    b2_x2, b2_x4 = b2_x3, b2_x1\n",
        "    b2_y1, b2_y3 = box2[:,1] - box2[:,4] / 2, box2[:,1] + box2[:,4] / 2\n",
        "    b2_y2, b2_y4 = b2_y1, b2_y3\n",
        "    #b2_z1, b2_z3 = box2[2] - box2[5] / 2, box2[2] + box2[5] / 2\n",
        "\n",
        "    rotation1 = [angle_decoder(r[6:8]) for r in box1]\n",
        "    rotation2 = [angle_decoder(r[6:8]) for r in box2]\n",
        "    \n",
        "\n",
        "\n",
        "    if box1.shape == box2.shape:\n",
        "      #print(\"in 1\")\n",
        "      for x11,x12,x13,x14,y11,y12,y13,y14,x21,x22,x23,x24,y21,y22,y23,y24,r1,r2, b1,b2 in zip(b1_x1,b1_x2,b1_x3,b1_x4, b1_y1,b1_y2,b1_y3,b1_y4, b2_x1,b2_x2,b2_x3,b2_x4, b2_y1,b2_y2,b2_y3,b2_y4, rotation1,rotation2, box1,box2):\n",
        "        p1 = Polygon([rotate_around_point((x11, y11), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x12, y12), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x13, y13), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x14, y14), r1, (b1[0], b1[1]))])\n",
        "        p2 = Polygon([rotate_around_point((x21, y21), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x22, y22), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x23, y23), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x24, y24), r2, (b2[0], b2[1]))])\n",
        "        p3 = p1.intersection(p2)\n",
        "        inter_area = p3.area\n",
        "        b1_area = p1.area\n",
        "        b2_area = p2.area\n",
        "\n",
        "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "      return torch.Tensor(iou_scores).to(device)\n",
        "    else:\n",
        "      #print(\"in 2\")\n",
        "      x11,x12,x13,x14, y11,y12,y13,y14 = b1_x1[0],b1_x2[0],b1_x3[0],b1_x4[0], b1_y1[0],b1_y2[0],b1_y3[0],b1_y4[0]\n",
        "      r1, b1 = rotation1[0], box1[0]\n",
        "      for x21,x22,x23,x24,y21,y22,y23,y24,r2 ,b2 in zip(b2_x1,b2_x2,b2_x3,b2_x4, b2_y1,b2_y2,b2_y3,b2_y4, rotation2, box2):\n",
        "        p1 = Polygon([rotate_around_point((x11, y11), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x12, y12), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x13, y13), r1, (b1[0], b1[1])),\n",
        "             rotate_around_point((x14, y14), r1, (b1[0], b1[1]))])\n",
        "        p2 = Polygon([rotate_around_point((x21, y21), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x22, y22), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x23, y23), r2, (b2[0], b2[1])),\n",
        "             rotate_around_point((x24, y24), r2, (b2[0], b2[1]))])\n",
        "        p3 = p1.intersection(p2)\n",
        "        inter_area = p3.area\n",
        "        b1_area = p1.area\n",
        "        b2_area = p2.area\n",
        "\n",
        "        iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
        "        iou_scores.append(iou)\n",
        "        #print(\"iou = \",iou,\"iou_scores.shape :\",len(iou_scores))\n",
        "\n",
        "      return torch.Tensor(iou_scores).to(device)\n",
        "\n",
        "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.5):\n",
        "    \"\"\"\n",
        "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
        "    Non-Maximum Suppression to further filter detections.\n",
        "    Returns detections with shape:\n",
        "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
        "    \"\"\"\n",
        "\n",
        "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "    output = [None for _ in range(len(prediction))]\n",
        "    for image_i, image_pred in enumerate(prediction):\n",
        "        # Filter out confidence scores below threshold\n",
        "        image_pred = image_pred[image_pred[:, 8] >= conf_thres]\n",
        "        # If none are remaining => process next image\n",
        "        if not image_pred.size(0):\n",
        "            continue\n",
        "        # Object confidence times class confidence\n",
        "        score = image_pred[:, 8] * image_pred[:, 9:].max(1)[0]\n",
        "        # Sort by it\n",
        "        image_pred = image_pred[(-score).argsort()]\n",
        "        class_confs, class_preds = image_pred[:, 9:].max(1, keepdim=True)\n",
        "        detections = torch.cat((image_pred[:, :9], class_confs.float(), class_preds.float()), 1)\n",
        "        # Perform non-maximum suppression\n",
        "        keep_boxes = []\n",
        "        while detections.size(0):\n",
        "            #print(\"bbox :\",bbox_iou(detections[0, :8].unsqueeze(0), detections[:, :8]))\n",
        "            large_overlap = bbox_iou(detections[0, :8].unsqueeze(0), detections[:, :8]) > nms_thres\n",
        "            #print(\"detection.size:\",detections.size())\n",
        "            label_match = detections[0, -1] == detections[:, -1]\n",
        "            label_match = label_match.cuda()\n",
        "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
        "            invalid = large_overlap & label_match\n",
        "            weights = detections[invalid, 8:9]\n",
        "            # Merge overlapping bboxes by order of confidence\n",
        "            detections[0, :8] = (weights * detections[invalid, :8]).sum(0) / weights.sum()\n",
        "            keep_boxes += [detections[0]]\n",
        "            detections = detections[~invalid]\n",
        "        if keep_boxes:\n",
        "            output[image_i] = torch.stack(keep_boxes)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n",
        "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
        "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
        "\n",
        "    nB = pred_boxes.size(0)\n",
        "    nA = pred_boxes.size(1)\n",
        "    nC = pred_cls.size(-1)\n",
        "    nG = pred_boxes.size(2)\n",
        "    \n",
        "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
        "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
        "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tz = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tl = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tr1 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tr2 = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
        "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
        "    \n",
        "    target_boxes = target[:, 2:10]\n",
        "    target_boxes[:, :2] = target_boxes[:, :2] * nG\n",
        "    target_boxes[:, 3:5] = target_boxes[:, 3:5] * nG\n",
        "    target_boxes[:, 5] = target_boxes[:, 5] * 5\n",
        "\n",
        "    gxy = target_boxes[:, :2]\n",
        "    gwlh = target_boxes[:, 3:6]\n",
        "\n",
        "    ious = torch.stack([bbox_wh_iou(anchor, gwlh) for anchor in anchors]) #here\n",
        "    try:\n",
        "      best_ious, best_n = ious.max(0)\n",
        "      #print(\"ious shape :\",ious.shape)\n",
        "      #print(\"ious :\", ious)\n",
        "      #print(\"Best_ious shape :\", best_ious.shape)\n",
        "      #print(\"Best_ious :\", best_ious)\n",
        "      #print(\"Best_n shape :\", best_n.shape)\n",
        "      #print(\"Best_n :\", best_n)\n",
        "\n",
        "\n",
        "    except:\n",
        "      best_ious, best_n = torch.LongTensor(0), torch.LongTensor(0)\n",
        "      #tconf = obj_mask.float()\n",
        "      #return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf\n",
        "      pass\n",
        "\n",
        "    b, target_labels = target[:, :2].long().t()\n",
        "    \n",
        "    gx, gy = gxy.t()\n",
        "    gw, gl, gh = gwlh.t()\n",
        "    gi, gj = gxy.long().t()\n",
        "    \n",
        "    # Set masks\n",
        "    obj_mask[b, best_n, gj, gi] = 1\n",
        "    noobj_mask[b, best_n, gj, gi] = 0\n",
        "\n",
        "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
        "    for i, anchor_ious in enumerate(ious.t()):\n",
        "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
        "\n",
        "    # Coordinates\n",
        "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
        "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
        "    tz[b, best_n, gj, gi] = target_boxes[:,2]\n",
        "    # Width and height\n",
        "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
        "    tl[b, best_n, gj, gi] = torch.log(gl / anchors[best_n][:, 1] + 1e-16)\n",
        "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 2] + 1e-16)\n",
        "    \n",
        "    tr1[b, best_n, gj, gi] = target_boxes[:,6]\n",
        "    tr2[b, best_n, gj, gi] = target_boxes[:,7]\n",
        "    # One-hot encoding of label\n",
        "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
        "    # Compute label correctness and iou at best anchor\n",
        "    try:\n",
        "      class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
        "    except:\n",
        "      pass\n",
        "    tconf = obj_mask.float()\n",
        "    target_boxes[:, :2] = target_boxes[:, :2] / nG\n",
        "    target_boxes[:, 3:5] = target_boxes[:, 3:5] / nG\n",
        "    target_boxes[:, 5] = target_boxes[:, 5] / 5\n",
        "    return class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfNuZWdMrK2w"
      },
      "source": [
        "#Parse_config\n",
        "\n",
        "def parse_model_config(path):\n",
        "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
        "    file = open(path, 'r')\n",
        "    lines = file.read().split('\\n')\n",
        "    lines = [x for x in lines if x and not x.startswith('#')]\n",
        "    lines = [x.rstrip().lstrip() for x in lines] # get rid of fringe whitespaces\n",
        "    module_defs = []\n",
        "    for line in lines:\n",
        "        if line.startswith('['): # This marks the start of a new block\n",
        "            module_defs.append({})\n",
        "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
        "            if module_defs[-1]['type'] == 'convolutional':\n",
        "                module_defs[-1]['batch_normalize'] = 0\n",
        "        else:\n",
        "            key, value = line.split(\"=\")\n",
        "            value = value.strip()\n",
        "            module_defs[-1][key.rstrip()] = value.strip()\n",
        "\n",
        "    return module_defs\n",
        "\n",
        "def parse_data_config(path):\n",
        "    \"\"\"Parses the data configuration file\"\"\"\n",
        "    options = dict()\n",
        "    options['gpus'] = '0,1,2,3'\n",
        "    options['num_workers'] = '10'\n",
        "    with open(path, 'r') as fp:\n",
        "        lines = fp.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line == '' or line.startswith('#'):\n",
        "            continue\n",
        "        key, value = line.split('=')\n",
        "        options[key.strip()] = value.strip()\n",
        "    return options\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xUCTALNYmpR"
      },
      "source": [
        "#Draw\n",
        "def draw(targets):\n",
        "  fig = plt.figure(figsize=(8,8))\n",
        "  ax = fig.add_subplot(111)\n",
        "  for target in targets:\n",
        "    rotation = angle_decoder(target[6:8]) + math.pi/2\n",
        "    length = target[3]\n",
        "    width = target[4]\n",
        "    x_temp, y_temp = rotate_around_point((target[0],-target[1]), -(rotation), origin=(target[0]-width/2, -target[1] - length/2))\n",
        "    x_offset, y_offset = x_temp - target[0], y_temp + target[1]\n",
        "    rectas = patches.Rectangle(xy=((target[0]-width/2) - x_offset, (-target[1] - length/2) - y_offset) ,width=width, height=length, angle = (rotation)*180/math.pi, linewidth=1, color='blue', fill=False)\n",
        "    ax.add_patch(rectas)\n",
        "    ax.scatter(target[0], -target[1], color = 'red', s=10)\n",
        "  ax.scatter(0.5, -0.5)\n",
        "  plt.xlim(0, 1)\n",
        "  plt.ylim(-1,0)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLTv_sBOJrxK"
      },
      "source": [
        "#Sampler\n",
        "import torch\n",
        "\n",
        "class ResumableRandomSampler(torch.utils.data.Sampler):\n",
        "    r\"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n",
        "    If with replacement, then user can specify :attr:`num_samples` to draw.\n",
        "    Arguments:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "        replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``\n",
        "        num_samples (int): number of samples to draw, default=`len(dataset)`. This argument\n",
        "            is supposed to be specified only when `replacement` is ``True``.\n",
        "        generator (Generator): Generator used in sampling.\n",
        "    \"\"\"\n",
        "    #data_source: Sized\n",
        "    #replacement: bool\n",
        "\n",
        "    def __init__(self, data_source):\n",
        "        self.data_source = data_source\n",
        "        self.generator = torch.Generator()\n",
        "        self.generator.manual_seed(47)\n",
        "        \n",
        "        self.perm_index = 0\n",
        "        self.perm = torch.randperm(self.num_samples, generator=self.generator)\n",
        "        \n",
        "    @property\n",
        "    def num_samples(self) -> int:\n",
        "        return len(self.data_source)\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.perm_index >= len(self.perm):\n",
        "            self.perm_index = 0\n",
        "            self.perm = torch.randperm(self.num_samples, generator=self.generator)\n",
        "            \n",
        "        while self.perm_index < len(self.perm):\n",
        "            self.perm_index += 1\n",
        "            yield self.perm[self.perm_index-1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "    \n",
        "    def get_state(self):\n",
        "        return {\"perm\": self.perm, \"perm_index\": self.perm_index, \"generator_state\": self.generator.get_state()}\n",
        "    \n",
        "    def set_state(self, state):\n",
        "        self.perm = state[\"perm\"]\n",
        "        self.perm_index = state[\"perm_index\"]\n",
        "        self.generator.set_state(state[\"generator_state\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0phLTHEsL0P"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pyquaternion import Quaternion\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def pad_to_square(img, pad_value):\n",
        "    c, h, w = img.shape\n",
        "    dim_diff = np.abs(h - w)\n",
        "    # (upper / left) padding and (lower / right) padding\n",
        "    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n",
        "    # Determine padding\n",
        "    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n",
        "    # Add padding\n",
        "    img = F.pad(img, pad, \"constant\", value=pad_value)\n",
        "\n",
        "    return img, pad\n",
        "\n",
        "\n",
        "def resize(image, size):\n",
        "    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_resize(images, min_size=288, max_size=448):\n",
        "    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]\n",
        "    images = F.interpolate(images, size=new_size, mode=\"nearest\")\n",
        "    return images\n",
        "\n",
        "\n",
        "class ImageFolder(Dataset):\n",
        "    def __init__(self, folder_path, img_size=1248):\n",
        "        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.files[index % len(self.files)]\n",
        "        # Extract image as PyTorch tensor\n",
        "        img = transforms.ToTensor()(Image.open(img_path))\n",
        "        # Pad to square resolution\n",
        "        img, _ = pad_to_square(img, 0)\n",
        "        # Resize\n",
        "        img = resize(img, self.img_size)\n",
        "\n",
        "        return img_path, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    def __init__(self, sample_mapping, img_size=1248, augment=True, multiscale=False, normalized_labels=False, max_height=5, max_length=20, max_width=20):\n",
        "       \n",
        "        self.img_size = img_size\n",
        "        self.max_objects = 100\n",
        "        self.augment = augment\n",
        "        self.multiscale = multiscale\n",
        "        self.normalized_labels = normalized_labels\n",
        "        self.min_size = self.img_size - 3 * 32\n",
        "        self.max_size = self.img_size + 3 * 32\n",
        "        self.batch_count = 0\n",
        "        self.sample_mapping = sample_mapping\n",
        "        self.file_path = 'data/'\n",
        "        \n",
        "\n",
        "        #get categories\n",
        "        self.categories = []\n",
        "        with open('data/v1.0-trainval/category.json') as f:\n",
        "          data = json.load(f)\n",
        "        for d in data:\n",
        "          self.categories.append(d['name'])\n",
        "        self.num_category = len(self.categories)\n",
        "        \n",
        "        #important items for tweking\n",
        "        self.max_height = max_height\n",
        "        self.max_width = max_width\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def quaternion_yaw(self, q: Quaternion) -> float:\n",
        "      \"\"\"\n",
        "      Calculate the yaw angle from a quaternion.\n",
        "      See https://en.wikipedia.org/wiki/Conversion_between_quaternions_and_Euler_angles.\n",
        "      :param q: Quaternion of interest.\n",
        "      :return: Yaw angle in radians.\n",
        "      \"\"\"\n",
        "\n",
        "      a = 2.0 * (q[0] * q[3] + q[1] * q[2])\n",
        "      b = 1.0 - 2.0 * (q[2] ** 2 + q[3] ** 2)\n",
        "\n",
        "      return np.arctan2(a, b)\n",
        "\n",
        "    def rotate_around_point_lowperf(self, point, radians, origin=(0, 0)):\n",
        "      \"\"\"Rotate a point around a given point.\n",
        "      \n",
        "      I call this the \"low performance\" version since it's recalculating\n",
        "      the same values more than once [cos(radians), sin(radians), x-ox, y-oy).\n",
        "      It's more readable than the next function, though.\n",
        "      \"\"\"\n",
        "      x, y = point\n",
        "      ox, oy = origin\n",
        "\n",
        "      qx = ox + math.cos(radians) * (x - ox) + math.sin(radians) * (y - oy)\n",
        "      qy = oy + -math.sin(radians) * (x - ox) + math.cos(radians) * (y - oy)\n",
        "\n",
        "      return qx, qy\n",
        "\n",
        "    def convert_to_top_corner(self, point):\n",
        "      \"\"\"Convert the position with respect to the top left corner\"\"\"\n",
        "      point[0] = self.max_length + point[0]\n",
        "      point[1] = self.max_width - point[1]\n",
        "      return point\n",
        "\n",
        "    def check_cameraregion(self,coordinates,cameras,sample,verbose = False):\n",
        "      \"\"\"To check if the coordinate of the object lies in blacked out camera region\n",
        "       and if so return flag as 0\"\"\"\n",
        "      if verbose:\n",
        "        print(\"Start of Checking for the point :\",coordinates)\n",
        "      angle = 0\n",
        "      flag = False\n",
        "      for camera in cameras:\n",
        "        flag = False\n",
        "        sample_data = nusc.get('sample_data',sample['data'][camera])\n",
        "        sensor = nusc.get('calibrated_sensor',sample_data['calibrated_sensor_token'])\n",
        "        if camera == 'CAM_BACK':\n",
        "          angle = 55/180 * math.pi\n",
        "        else :\n",
        "          angle = 35/180 * math.pi\n",
        "        if verbose:\n",
        "          print(\"Sensor before rotation:\", sensor['translation'][0:2])\n",
        "        x, y = self.rotate_around_point_lowperf(sensor['translation'][0:2], -math.pi/2)\n",
        "        if verbose:\n",
        "          print(\"Sensor after rotation:\", x,y)\n",
        "        rotation = self.quaternion_yaw(sensor['rotation']) +math.pi\n",
        "        vl = coordinates[1] - y - math.tan(rotation+angle) * (coordinates[0]-x)\n",
        "        vr = coordinates[1] - y - math.tan(rotation-angle) * (coordinates[0]-x)\n",
        "        if verbose:\n",
        "          print(camera,\"Angles :\",(rotation+angle)*180/math.pi,(rotation-angle)*180/math.pi)\n",
        "        if (rotation + angle >= math.pi/2 and rotation + angle <= math.pi*3/2):\n",
        "          if vl >= 0:\n",
        "            flag = True\n",
        "        else:\n",
        "          if vl <= 0:\n",
        "            flag = True\n",
        "        if flag:\n",
        "          if (rotation - angle >= math.pi/2 and rotation - angle <= math.pi*3/2):\n",
        "            if vr <= 0:\n",
        "              flag = True\n",
        "              break\n",
        "            else:\n",
        "              flag = False\n",
        "          else:\n",
        "            if vr >= 0:\n",
        "              flag = True\n",
        "              break\n",
        "            else:\n",
        "              flag = False\n",
        "      return flag\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        token = self.sample_mapping[index]\n",
        "        my_sample = nusc.get('sample',token)\n",
        "        \n",
        "        # ---------\n",
        "        #  Image\n",
        "        # ---------\n",
        "\n",
        "        front = ['CAM_FRONT_LEFT','CAM_FRONT','CAM_FRONT_RIGHT']\n",
        "        back = ['CAM_BACK_RIGHT','CAM_BACK','CAM_BACK_LEFT']\n",
        "\n",
        "        camera = []\n",
        "        blackout_cameras = []\n",
        "\n",
        "        #Augmentation using camera blackout\n",
        "        if self.augment:\n",
        "          if np.random.random() < 0.3:\n",
        "            camera = front + back\n",
        "            numbers = [1,2,3,4,5]\n",
        "            number = random.choice(numbers)\n",
        "            blackout_cameras = random.sample(camera,number)\n",
        "\n",
        "        #Camera image stitching and applying blackout from the selected cameras\n",
        "        for f,b in zip(front,back):\n",
        "          sensorf = nusc.get('sample_data',my_sample['data'][f])\n",
        "          sensorb = nusc.get('sample_data',my_sample['data'][b])\n",
        "\n",
        "          if f == 'CAM_FRONT_LEFT' and b == 'CAM_BACK_RIGHT':\n",
        "            image_dataf = transforms.ToTensor()(Image.open(self.file_path + sensorf['filename']).convert('RGB'))\n",
        "            if 'CAM_FRONT_LEFT' in blackout_cameras:\n",
        "              image_dataf = torch.rand(image_dataf.shape)\n",
        "            image_datab = transforms.ToTensor()(Image.open(self.file_path + sensorb['filename']).convert('RGB'))\n",
        "            if 'CAM_BACK_RIGHT' in blackout_cameras:\n",
        "              image_datab = torch.rand(image_dataf.shape)\n",
        "          else:\n",
        "            data = transforms.ToTensor()(Image.open(self.file_path + sensorf['filename']).convert('RGB'))\n",
        "            if f in blackout_cameras:\n",
        "              data = torch.rand(data.shape)\n",
        "            image_dataf = torch.cat((image_dataf,data),2)\n",
        "            data = transforms.ToTensor()(Image.open(self.file_path + sensorb['filename']).convert('RGB'))\n",
        "            if b in blackout_cameras:\n",
        "              data = torch.rand(data.shape)\n",
        "            image_datab = torch.cat((image_datab,data),2)\n",
        "        #fliping the bottem image for more consistency\n",
        "        image_datab = torch.flip(image_datab, [-1])\n",
        "        #concatinating the top and bottem image\n",
        "        image_data = torch.cat((image_dataf,image_datab),1)\n",
        "        #resizeing the image to square\n",
        "        image_data, _ = pad_to_square(image_data,0)\n",
        "\n",
        "        \n",
        "        # ---------\n",
        "        #  Label\n",
        "        # ---------\n",
        "\n",
        "        l_factor, w_factor, h_factor = (self.max_length, self.max_width, self.max_height) if self.normalized_labels else (1, 1, 1)\n",
        "        targets = None\n",
        "\n",
        "\n",
        "        annos_list = my_sample['anns']\n",
        "        converted_anotations = []\n",
        "        #ego pose\n",
        "        sensor = 'LIDAR_TOP'\n",
        "        lidar_top_data = nusc.get('sample_data', my_sample['data'][sensor])\n",
        "        ego_pose = nusc.get('ego_pose', lidar_top_data['ego_pose_token'])\n",
        "        ego_yaw = self.quaternion_yaw(ego_pose['rotation']) - math.pi/2\n",
        "\n",
        "        boxes = []\n",
        "        t=[]\n",
        "        original_ego_yaw = ego_yaw + math.pi/2 #converting back to original value\n",
        "\n",
        "        for annos in annos_list:\n",
        "          annotation = nusc.get('sample_annotation', annos)\n",
        "          vis = nusc.get('visibility',annotation['visibility_token'])\n",
        "          vis = vis['level'][1:].split(\"-\")\n",
        "          if int(vis[1]) <= 40 :\n",
        "            continue\n",
        "          #print(vis)\n",
        "          box = []\n",
        "\n",
        "          #xyz\n",
        "          flag = False\n",
        "          cordinates = [annotation['translation'][i] - ego_pose['translation'][i] for i in range(3)]\n",
        "          cordinates[0], cordinates[1] = self.rotate_around_point_lowperf(cordinates[:2], ego_yaw, origin=(0, 0))\n",
        "          if self.augment :\n",
        "            flag = self.check_cameraregion(cordinates,blackout_cameras,my_sample)\n",
        "          cordinates = self.convert_to_top_corner(cordinates)\n",
        "          if cordinates[0] > 2*self.max_width or cordinates[0] < 0 or cordinates[1] > 2*self.max_length or cordinates[1] < 0 or flag:# or (self.augment and self.check_cameraregion() == 0):\n",
        "            continue\n",
        "\n",
        "          #whl\n",
        "          size = annotation['size']\n",
        "\n",
        "          #angle r1, r2\n",
        "            #converting to relative angle (0-360)\n",
        "          rotation_yaw = self.quaternion_yaw(annotation['rotation']) - original_ego_yaw\n",
        "          if rotation_yaw < 0:\n",
        "            rotation_yaw += math.pi*2\n",
        "          r1 = (1 + math.sin(rotation_yaw))/2\n",
        "          r2 = (1 + math.cos(rotation_yaw))/2\n",
        "\n",
        "          #category\n",
        "          category_index = self.categories.index(annotation['category_name'])\n",
        "\n",
        "          #Appending to Box\n",
        "          box.append(category_index)\n",
        "          for i,j in zip(cordinates, [self.max_width*2, self.max_length*2, self.max_height]):\n",
        "            box.append(i/j)\n",
        "          for i,j in zip(size, [self.max_width*2, self.max_length*2, self.max_height]):\n",
        "            box.append(i/j)\n",
        "          box.append(r1)\n",
        "          box.append(r2)\n",
        "\n",
        "          #Appending to Boxes\n",
        "          boxes.append(box)\n",
        "          t.append(annos)\n",
        "\n",
        "        boxes = torch.Tensor(boxes)\n",
        "\n",
        "        targets = torch.zeros((len(boxes), 10))\n",
        "        if len(boxes)> 0:\n",
        "          targets[:, 1:] = boxes\n",
        "\n",
        "        # Apply augmentations\n",
        "        #if self.augment:\n",
        "        #  image_data, targets = horisontal_flip(image_data, targets, Verbose = True)\n",
        "        #    if np.random.random() < 0.5:\n",
        "        return image_data, targets\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        imgs, targets = list(zip(*batch))\n",
        "        # Remove empty placeholder targets\n",
        "        targets = [boxes for boxes in targets if boxes is not None]\n",
        "        # Add sample index to targets\n",
        "        for i, boxes in enumerate(targets):\n",
        "            targets[i][:, 0] = i\n",
        "        targets = torch.cat(targets, 0)\n",
        "        \"\"\"\n",
        "        # Selects new image size every tenth batch\n",
        "        if self.multiscale and self.batch_count % 10 == 0:\n",
        "            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n",
        "        \"\"\"\n",
        "        # Resize images to input shape\n",
        "        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n",
        "        self.batch_count += 1\n",
        "        \n",
        "        return imgs, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_mapping)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "takpM5RvrupZ"
      },
      "source": [
        "#MODEL.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDDUlseGjmRJ"
      },
      "source": [
        "#MODEL\n",
        "\n",
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "\n",
        "def create_modules(module_defs):\n",
        "    \"\"\"\n",
        "    Constructs module list of layer blocks from module configuration in module_defs\n",
        "    \"\"\"\n",
        "    hyperparams = module_defs.pop(0)\n",
        "    output_filters = [int(hyperparams[\"channels\"])]\n",
        "    module_list = nn.ModuleList()\n",
        "    for module_i, module_def in enumerate(module_defs):\n",
        "        modules = nn.Sequential()\n",
        "\n",
        "        if module_def[\"type\"] == \"convolutional\":\n",
        "            bn = int(module_def[\"batch_normalize\"])\n",
        "            filters = int(module_def[\"filters\"])\n",
        "            kernel_size = int(module_def[\"size\"])\n",
        "            pad = (kernel_size - 1) // 2\n",
        "            modules.add_module(\n",
        "                f\"conv_{module_i}\",\n",
        "                nn.Conv2d(\n",
        "                    in_channels=output_filters[-1],\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=int(module_def[\"stride\"]),\n",
        "                    padding=pad,\n",
        "                    bias=not bn,\n",
        "                ),\n",
        "            )\n",
        "            if bn:\n",
        "                modules.add_module(f\"batch_norm_{module_i}\", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n",
        "            if module_def[\"activation\"] == \"leaky\":\n",
        "                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n",
        "\n",
        "        elif module_def[\"type\"] == \"maxpool\":\n",
        "            kernel_size = int(module_def[\"size\"])\n",
        "            stride = int(module_def[\"stride\"])\n",
        "            if kernel_size == 2 and stride == 1:\n",
        "                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n",
        "            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n",
        "            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n",
        "\n",
        "        elif module_def[\"type\"] == \"upsample\":\n",
        "            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n",
        "            modules.add_module(f\"upsample_{module_i}\", upsample)\n",
        "\n",
        "        elif module_def[\"type\"] == \"route\":\n",
        "            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n",
        "            filters = sum([output_filters[1:][i] for i in layers])\n",
        "            modules.add_module(f\"route_{module_i}\", EmptyLayer())\n",
        "\n",
        "        elif module_def[\"type\"] == \"shortcut\":\n",
        "            filters = output_filters[1:][int(module_def[\"from\"])]\n",
        "            modules.add_module(f\"shortcut_{module_i}\", EmptyLayer())\n",
        "\n",
        "        elif module_def[\"type\"] == \"yolo\":\n",
        "            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n",
        "            # Extract anchors\n",
        "            anchors = [float(x) for x in module_def[\"anchors\"].split(\",\")]\n",
        "            anchors = [(anchors[i], anchors[i + 1], anchors[i + 2]) for i in range(0, len(anchors), 3)]\n",
        "            anchors = [anchors[i] for i in anchor_idxs]\n",
        "            num_classes = int(module_def[\"classes\"])\n",
        "            img_size = int(hyperparams[\"height\"])\n",
        "            # Define detection layer\n",
        "            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n",
        "            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n",
        "        # Register module list and number of output filters\n",
        "        module_list.append(modules)\n",
        "        output_filters.append(filters)\n",
        "\n",
        "    return hyperparams, module_list\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\" nn.Upsample is deprecated \"\"\"\n",
        "\n",
        "    def __init__(self, scale_factor, mode=\"nearest\"):\n",
        "        super(Upsample, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EmptyLayer(nn.Module):\n",
        "    \"\"\"Placeholder for 'route' and 'shortcut' layers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EmptyLayer, self).__init__()\n",
        "\n",
        "\n",
        "class YOLOLayer(nn.Module):\n",
        "    \"\"\"Detection layer\"\"\"\n",
        "\n",
        "    def __init__(self, anchors, num_classes, img_dim=1248):\n",
        "        super(YOLOLayer, self).__init__()\n",
        "        self.anchors = anchors\n",
        "        self.num_anchors = len(anchors)\n",
        "        self.num_classes = num_classes\n",
        "        self.ignore_thres = 0.5\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "        self.obj_scale = 1\n",
        "        self.noobj_scale = 100\n",
        "        self.metrics = {}\n",
        "        self.img_dim = img_dim\n",
        "        self.grid_size = 0  # grid size\n",
        "\n",
        "    def compute_grid_offsets(self, grid_size, cuda=True):\n",
        "        self.grid_size = grid_size\n",
        "        g = self.grid_size\n",
        "        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "        self.stride = self.img_dim / self.grid_size\n",
        "        # Calculate offsets for each grid\n",
        "        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n",
        "        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n",
        "        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_l / self.stride, a_h ) for a_w, a_l, a_h in self.anchors])\n",
        "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
        "        self.anchor_l = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
        "        self.anchor_h = self.scaled_anchors[:, 2:3].view((1, self.num_anchors, 1, 1))\n",
        "\n",
        "    def forward(self, x, targets=None, img_dim=None):\n",
        "\n",
        "        # Tensors for cuda support\n",
        "        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n",
        "        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n",
        "        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n",
        "\n",
        "        self.img_dim = img_dim\n",
        "        num_samples = x.size(0)\n",
        "        grid_size = x.size(2)\n",
        "        prediction = (\n",
        "            x.view(num_samples, self.num_anchors, self.num_classes + 9, grid_size, grid_size)\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "            .contiguous()\n",
        "        )\n",
        "\n",
        "        # Get outputs\n",
        "        x = torch.sigmoid(prediction[..., 0])  # Center x\n",
        "        y = torch.sigmoid(prediction[..., 1])  # Center y\n",
        "        z = torch.sigmoid(prediction[..., 2])  # Center z\n",
        "        l = prediction[..., 3]  # Length\n",
        "        w = prediction[..., 4]  # Width\n",
        "        h = prediction[..., 5]  # Height\n",
        "        r1 = torch.sigmoid(prediction[..., 6])  # Rotation r1\n",
        "        r2 = torch.sigmoid(prediction[..., 7])  # Rotation r2\n",
        "        pred_conf = torch.sigmoid(prediction[..., 8])  # Conf\n",
        "        pred_cls = torch.sigmoid(prediction[..., 9:])  # Cls pred.\n",
        "        \n",
        "        # If grid size does not match current we compute new offsets\n",
        "        if grid_size != self.grid_size:\n",
        "            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n",
        "        # Add offset and scale with anchors\n",
        "        pred_boxes = FloatTensor(prediction[..., :8].shape)\n",
        "        pred_boxes[..., 0] = x.data + self.grid_x\n",
        "        pred_boxes[..., 1] = y.data + self.grid_y\n",
        "        pred_boxes[..., 2] = z.data\n",
        "        pred_boxes[..., 3] = torch.exp(w.data) * self.anchor_w\n",
        "        pred_boxes[..., 4] = torch.exp(l.data) * self.anchor_l\n",
        "        pred_boxes[..., 5] = torch.exp(h.data) * self.anchor_h\n",
        "        pred_boxes[..., 6] = r1.data\n",
        "        pred_boxes[..., 7] = r2.data\n",
        "\n",
        "        replacement = pred_boxes.view(num_samples, -1, 8)\n",
        "        replacement[...,:2] = replacement[...,:2] * self.stride\n",
        "        replacement[...,2] = replacement[...,2] * 5\n",
        "        replacement[...,3:5] = replacement[...,3:5] * self.stride\n",
        "        replacement[...,5] = replacement[...,5]\n",
        "        replacement[...,6] = replacement[...,6]\n",
        "        replacement[...,7] = replacement[...,7]\n",
        "\n",
        "\n",
        "        output = torch.cat(\n",
        "            (\n",
        "                replacement,\n",
        "                pred_conf.view(num_samples, -1, 1),\n",
        "                pred_cls.view(num_samples, -1, self.num_classes),\n",
        "            ),\n",
        "            -1,\n",
        "        )\n",
        "\n",
        "        if targets is None:\n",
        "            return output, 0\n",
        "        else:\n",
        "            class_mask, obj_mask, noobj_mask, tx, ty, tz, tw, tl, th, tr1, tr2, tcls, tconf = build_targets(\n",
        "                pred_boxes=pred_boxes,\n",
        "                pred_cls=pred_cls,\n",
        "                target=targets,\n",
        "                anchors=self.scaled_anchors,\n",
        "                ignore_thres=self.ignore_thres,\n",
        "            )\n",
        "            # Loss : Mask outputs to ignore non-existing objects (except with conf. loss)\n",
        "            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
        "            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
        "            loss_z = self.mse_loss(z[obj_mask], tz[obj_mask])\n",
        "            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
        "            loss_l = self.mse_loss(l[obj_mask], tl[obj_mask])\n",
        "            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
        "            loss_r1 = self.mse_loss(r1[obj_mask], tr1[obj_mask])\n",
        "            loss_r2 = self.mse_loss(r2[obj_mask], tr2[obj_mask])\n",
        "            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
        "            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
        "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
        "            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n",
        "            total_loss = loss_x + loss_y + loss_z + loss_w + loss_l + loss_h + loss_r1 + loss_r2 + loss_conf + loss_cls\n",
        "\n",
        "\n",
        "            return output, total_loss\n",
        "\n",
        "\n",
        "class Darknet(nn.Module):\n",
        "    \"\"\"YOLOv3 object detection model\"\"\"\n",
        "\n",
        "    def __init__(self, config_path, img_size=1248):\n",
        "        super(Darknet, self).__init__()\n",
        "        self.module_defs = parse_model_config(config_path)\n",
        "        self.hyperparams, self.module_list = create_modules(self.module_defs)\n",
        "        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], \"metrics\")]\n",
        "        self.img_size = img_size\n",
        "        self.seen = 0\n",
        "        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        img_dim = x.shape[2]\n",
        "        loss = 0\n",
        "        layer_outputs, yolo_outputs = [], []\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
        "            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
        "                x = module(x)\n",
        "            elif module_def[\"type\"] == \"route\":\n",
        "                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n",
        "            elif module_def[\"type\"] == \"shortcut\":\n",
        "                layer_i = int(module_def[\"from\"])\n",
        "                x = layer_outputs[-1] + layer_outputs[layer_i]\n",
        "            elif module_def[\"type\"] == \"yolo\":\n",
        "                x, layer_loss = module[0](x, targets, img_dim)\n",
        "                loss += layer_loss\n",
        "                yolo_outputs.append(x)\n",
        "            layer_outputs.append(x)\n",
        "        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n",
        "        return yolo_outputs if targets is None else (loss, yolo_outputs)\n",
        "\n",
        "    def load_darknet_weights(self, weights_path):\n",
        "        \"\"\"Parses and loads the weights stored in 'weights_path'\"\"\"\n",
        "\n",
        "        # Open the weights file\n",
        "        with open(weights_path, \"rb\") as f:\n",
        "            header = np.fromfile(f, dtype=np.int32, count=5)  # First five are header values\n",
        "            self.header_info = header  # Needed to write header when saving weights\n",
        "            self.seen = header[3]  # number of images seen during training\n",
        "            weights = np.fromfile(f, dtype=np.float32)  # The rest are weights\n",
        "\n",
        "        # Establish cutoff for loading backbone weights\n",
        "        cutoff = None\n",
        "        if \"darknet53.conv.74\" in weights_path:\n",
        "            cutoff = 75\n",
        "\n",
        "        ptr = 0\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n",
        "            if i == cutoff:\n",
        "                break\n",
        "            if module_def[\"type\"] == \"convolutional\":\n",
        "                conv_layer = module[0]\n",
        "                if module_def[\"batch_normalize\"]:\n",
        "                    # Load BN bias, weights, running mean and running variance\n",
        "                    bn_layer = module[1]\n",
        "                    num_b = bn_layer.bias.numel()  # Number of biases\n",
        "                    # Bias\n",
        "                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n",
        "                    bn_layer.bias.data.copy_(bn_b)\n",
        "                    ptr += num_b\n",
        "                    # Weight\n",
        "                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n",
        "                    bn_layer.weight.data.copy_(bn_w)\n",
        "                    ptr += num_b\n",
        "                    # Running Mean\n",
        "                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n",
        "                    bn_layer.running_mean.data.copy_(bn_rm)\n",
        "                    ptr += num_b\n",
        "                    # Running Var\n",
        "                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n",
        "                    bn_layer.running_var.data.copy_(bn_rv)\n",
        "                    ptr += num_b\n",
        "                else:\n",
        "                    # Load conv. bias\n",
        "                    num_b = conv_layer.bias.numel()\n",
        "                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n",
        "                    conv_layer.bias.data.copy_(conv_b)\n",
        "                    ptr += num_b\n",
        "                # Load conv. weights\n",
        "                num_w = conv_layer.weight.numel()\n",
        "                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n",
        "                conv_layer.weight.data.copy_(conv_w)\n",
        "                ptr += num_w\n",
        "\n",
        "    def save_darknet_weights(self, path, cutoff=-1):\n",
        "        \"\"\"\n",
        "            @:param path    - path of the new weights file\n",
        "            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n",
        "        \"\"\"\n",
        "        fp = open(path, \"wb\")\n",
        "        self.header_info[3] = self.seen\n",
        "        self.header_info.tofile(fp)\n",
        "\n",
        "        # Iterate through layers\n",
        "        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n",
        "            if module_def[\"type\"] == \"convolutional\":\n",
        "                conv_layer = module[0]\n",
        "                # If batch norm, load bn first\n",
        "                if module_def[\"batch_normalize\"]:\n",
        "                    bn_layer = module[1]\n",
        "                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n",
        "                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n",
        "                # Load conv bias\n",
        "                else:\n",
        "                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n",
        "                # Load conv weights\n",
        "                conv_layer.weight.data.cpu().numpy().tofile(fp)\n",
        "\n",
        "        fp.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t51RsFcXEMHk"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUGbtD3-FLvV"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def evaluate(model, data_samples, iou_thres, conf_thres, nms_thres, img_size, batch_size):\n",
        "    model.eval()\n",
        "\n",
        "    # Get dataloader\n",
        "    dataset = ListDataset(data_samples, img_size=img_size, augment=False, multiscale=False)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "    labels = []\n",
        "    sample_metrics = []  # List of tuples (TP, confs, pred)\n",
        "    for batch_i, (imgs, targets) in enumerate(tqdm.tqdm(dataloader, desc=\"Detecting objects\")):\n",
        "\n",
        "        # Extract labels\n",
        "        labels += targets[:, 1].tolist()\n",
        "        # Rescale target\n",
        "        targets[:, 2:8] *= img_size\n",
        "\n",
        "        imgs = Variable(imgs.type(Tensor), requires_grad=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(imgs)\n",
        "            outputs = non_max_suppression(outputs, conf_thres=conf_thres, nms_thres=nms_thres)\n",
        "\n",
        "        sample_metrics += get_batch_statistics(outputs, targets, iou_threshold=iou_thres)\n",
        "        #print(sample_metrics)\n",
        "        del imgs,targets,outputs\n",
        "        torch.cuda.empty_cache()\n",
        "    # Concatenate sample statistics\n",
        "    if not sample_metrics :\n",
        "      precision, recall, AP, f1, ap_class = np.asarray([0]),np.asarray([0]),np.asarray([0]),np.asarray([0]),[]\n",
        "    else :\n",
        "      true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
        "      precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
        "\n",
        "    return precision, recall, AP, f1, ap_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-CRslW0Xujc"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0zm8fD5ISjg"
      },
      "source": [
        "class parameters:\n",
        "  def __init__(self, pretrained_weights = \"weights/darknet53.conv.74\", epochs = 100, batch_size = 8, step_size = 4, model_def = \"config/yolov3custom.cfg\", n_cpu = 8, img_size = 416, checkpoint_interval = 1, evaluation_interval = 5, compute_map = False, multiscale_training = True):\n",
        "    self.epochs = epochs #number of epochs\n",
        "    self.batch_size = batch_size #size of each image batch\n",
        "    self.gradient_accumulations =step_size    #number of gradient accums before step\n",
        "    self.model_def = model_def #path to model definition file\n",
        "    #self.data_config = data_config #path to data config file`\n",
        "    self.pretrained_weights = pretrained_weights #if specified starts from checkpoint model\n",
        "    self.n_cpu = n_cpu #number of cpu threads to use during batch generation\n",
        "    self.img_size = img_size #size of each image dimension\n",
        "    self.checkpoint_interval = checkpoint_interval #interval between saving model weights\n",
        "    self.evaluation_interval = evaluation_interval #interval evaluations on validation set\n",
        "    self.compute_map = compute_map #if True computes mAP every tenth batch\n",
        "    self.multiscale_training = multiscale_training #allow for multi-scale training\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oI0p5QnYyK2"
      },
      "source": [
        "opt = parameters(epochs = 3, pretrained_weights = \"checkpoints/aws-short20-3.pth\",img_size = 1248,batch_size = 2,evaluation_interval = 1,checkpoint_interval=50,step_size=8)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPTzU1mCavSJ",
        "outputId": "a5259aee-0e59-4d9d-f0b2-dc4ed637ce77"
      },
      "source": [
        "model = Darknet(opt.model_def).to(device)\n",
        "model.apply(weights_init_normal) #applying initial weights to the model. Since it is using module apply recursively goes through each layer and initializes the specified weights weights_init_normal is a function in utils\n",
        "# If specified we start from checkpoint\n",
        "if opt.pretrained_weights:\n",
        "  if opt.pretrained_weights.endswith(\".pth\"):\n",
        "    model.load_state_dict(torch.load(opt.pretrained_weights))\n",
        "    print(\"Model LOADING Finished !!!\")\n",
        "  else:\n",
        "    model.load_darknet_weights(opt.pretrained_weights) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model LOADING Finished !!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIGSAkmDK3af",
        "outputId": "3f1a7c29-f201-45da-cc50-794cb6d1aa62"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sample_mapping = []\n",
        "file_path = 'data/'\n",
        "for scene  in nusc.scene :\n",
        "  sample_token = scene['first_sample_token']\n",
        "  sample = nusc.get('sample',sample_token)\n",
        "  while not sample['next'] =='':\n",
        "    new_token = sample['token']\n",
        "    sample_mapping.append(new_token)\n",
        "    sample = nusc.get('sample',sample['next'])\n",
        "\n",
        "train_samples, test_samples = train_test_split(sample_mapping, test_size=0.2, random_state=42)\n",
        "print(len(train_samples),len(test_samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26639 6660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tAWjHQVsV9D"
      },
      "source": [
        "dataset = ListDataset(train_samples,  augment=False, multiscale=opt.multiscale_training)\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    shuffle = True,\n",
        "    batch_size=opt.batch_size,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=dataset.collate_fn,\n",
        ")\n",
        "class_names = dataset.categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLdpDzAWZM0D"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr =0.0005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bd15af9a685a4c268890765c47b979d9",
            "cb492cf1a7894c788d8768c8f4cc0060",
            "6f714e6fe57a4e0da5bc1fb5a340b171",
            "6434d127fc2946f094813511710e3f2d",
            "b2ff5dbeed07463f98bad70e0ce3a39e",
            "9071d2727fbd4e94b0bdc7c339d6a972",
            "2d6f27faa36a4d4eb18215a888777325",
            "6a518f40b2bb4d749a404dec540ed358"
          ]
        },
        "id": "4tGdtKinQ-ll",
        "outputId": "f1e1a177-0d45-4231-dcec-0e4aaf46170c"
      },
      "source": [
        "import tqdm\n",
        "import json\n",
        "\n",
        "#\n",
        "if os.path.exists(\"checkpoints/step-short20.json\"):\n",
        "  with open('checkpoints/step-short20.json') as f:\n",
        "    data = json.load(f)\n",
        "  step = data['step']\n",
        "else:\n",
        "  step = 0\n",
        "for epoch in tqdm.notebook.tqdm(range(opt.epochs),unit='epoch'):\n",
        "    model.train()\n",
        "    for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "        batches_done = len(dataloader) * epoch + batch_i\n",
        "        \n",
        "        imgs = Variable(imgs.to(device))\n",
        "        targets = Variable(targets.to(device), requires_grad=False)\n",
        "\n",
        "        loss, outputs = model(imgs, targets)\n",
        "        loss.backward()\n",
        "        loss = loss.cpu().detach().numpy()\n",
        "\n",
        "        try:\n",
        "          print(\"Step :\",step,\" -- \",batch_i,\"Batch\",loss,\"Targets = \",targets.shape[0],\"Ratio =\",loss/targets.shape[0],\"Threshold (0.5):\")#, len(non_max_suppression(outputs,conf_thres = 0.5,nms_thres=0.5)[0]) ,\",\", len(non_max_suppression(outputs,conf_thres = 0.5,nms_thres=0.5)[1]))\n",
        "        except:\n",
        "          print(\"Step :\",step,\" -- \",batch_i,\"Batch\",loss,\"Targets = \",targets.shape[0],\"Ratio =\",loss/targets.shape[0])\n",
        "          \n",
        "        if batches_done % opt.gradient_accumulations == 0 and batches_done != 0:\n",
        "            # Accumulates gradient before each step\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            print(\"->\",optimizer.param_groups[0]['lr'])\n",
        "            step += 1\n",
        "            steps ={'step':step}\n",
        "            if step == 400000:\n",
        "              optimizer.param_groups[0]['lr'] = 0.0001\n",
        "            elif step ==  450000 :\n",
        "              optimizer.param_groups[0]['lr'] = 0.00001\n",
        "\n",
        "        \n",
        "        if batch_i % opt.checkpoint_interval == 0 and batch_i != 0:\n",
        "            torch.save(model.state_dict(), \"checkpoints/aws-short20-3.pth\")\n",
        "            with open(\"checkpoints/step-short20.json\",'w+') as f:\n",
        "              json.dump(steps,f)\n",
        "            print(\"Saved\")\n",
        "            dpi = 80\n",
        "            im_data = imgs[0].cpu().permute(1, 2, 0)\n",
        "            height, width, depth = im_data.size()\n",
        "\n",
        "            # What size does the figure need to be in inches to fit the image?\n",
        "            figsize = width / float(dpi), height / float(dpi)\n",
        "\n",
        "            # Create a figure of the right size with one axes that takes up the full figure\n",
        "            fig = plt.figure(figsize=figsize)\n",
        "            ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "            # Hide spines, ticks, etc.\n",
        "            ax.axis('off')\n",
        "\n",
        "            # Display the image.\n",
        "            ax.imshow(im_data, cmap='gray')\n",
        "\n",
        "            plt.show()\n",
        "            try:\n",
        "                out = non_max_suppression(outputs,conf_thres = 0.7,nms_thres=0.08)\n",
        "                out[0][:,:2] /=1024\n",
        "                out[0][:,3:5] /=1024\n",
        "                draw(out[0])\n",
        "                print(\"threshold :\",0.7)\n",
        "            except:\n",
        "                try:\n",
        "                    out = non_max_suppression(outputs,conf_thres = 0.6,nms_thres=0.05)\n",
        "                    out[0][:,:2] /=1024\n",
        "                    out[0][:,3:5] /=1024\n",
        "                    draw(out[0])\n",
        "                    print(\"threshold :\",0.4)\n",
        "                except:\n",
        "                  print(\"Nothing !!!!\")\n",
        "\n",
        "        del imgs,targets,outputs,loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        \n",
        "    if epoch % opt.evaluation_interval == 0 :#and epoch != 0:\n",
        "        print(\"\\n---- Evaluating Model ----\")\n",
        "        # Evaluate the model on the validation set\n",
        "        precision, recall, AP, f1, ap_class = evaluate(\n",
        "            model,\n",
        "            test_samples,\n",
        "            iou_thres=0.5,\n",
        "            conf_thres=0.5,\n",
        "            nms_thres=0.02,\n",
        "            img_size=opt.img_size,\n",
        "            batch_size=2,\n",
        "        )\n",
        "        evaluation_metrics = [\n",
        "            (\"val_precision\", precision.mean()),\n",
        "            (\"val_recall\", recall.mean()),\n",
        "            (\"val_mAP\", AP.mean()),\n",
        "            (\"val_f1\", f1.mean()),\n",
        "        ]\n",
        "        #logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
        "\n",
        "        # Print class APs and mAP\n",
        "        ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "        for i, c in enumerate(ap_class):\n",
        "            ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "        print(AsciiTable(ap_table).table)\n",
        "        print(f\"---- mAP {AP.mean()}\")\n",
        "        with open(\"checkpoints/mAP.txt\",'a') as f:\n",
        "            f.write(AsciiTable(ap_table).table)\n",
        "            f.write('\\nmAP : {map}\\n'.format(map = AP.mean()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0DB9iZRbY2y"
      },
      "source": [
        "with open(\"checkpoints/mAP.txt\",'a') as f:\n",
        "  f.write(AsciiTable(ap_table).table)\n",
        "  f.write('\\nmAP : {map}'.format(map = AP.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2_WZsuGhnL_",
        "outputId": "7b2620f5-9dbb-499e-eef2-27876e45f8ca"
      },
      "source": [
        "!cat checkpoints/mAP.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mAP : 0.00019884741225546839\r\n",
            "mAP : 3.461213096768677e-05\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| Index | Class name                           | AP      |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| 0     | human.pedestrian.adult               | 0.00001 |\r\n",
            "| 1     | human.pedestrian.child               | 0.00000 |\r\n",
            "| 2     | human.pedestrian.wheelchair          | 0.00000 |\r\n",
            "| 3     | human.pedestrian.stroller            | 0.00000 |\r\n",
            "| 4     | human.pedestrian.personal_mobility   | 0.00000 |\r\n",
            "| 5     | human.pedestrian.police_officer      | 0.00000 |\r\n",
            "| 6     | human.pedestrian.construction_worker | 0.00000 |\r\n",
            "| 7     | animal                               | 0.00000 |\r\n",
            "| 8     | vehicle.car                          | 0.00017 |\r\n",
            "| 9     | vehicle.motorcycle                   | 0.00004 |\r\n",
            "| 10    | vehicle.bicycle                      | 0.00000 |\r\n",
            "| 11    | vehicle.bus.bendy                    | 0.00000 |\r\n",
            "| 12    | vehicle.bus.rigid                    | 0.00000 |\r\n",
            "| 13    | vehicle.truck                        | 0.00103 |\r\n",
            "| 14    | vehicle.construction                 | 0.00000 |\r\n",
            "| 16    | vehicle.emergency.police             | 0.00000 |\r\n",
            "| 17    | vehicle.trailer                      | 0.00000 |\r\n",
            "| 18    | movable_object.barrier               | 0.00000 |\r\n",
            "| 19    | movable_object.trafficcone           | 0.00000 |\r\n",
            "| 20    | movable_object.pushable_pullable     | 0.00000 |\r\n",
            "| 21    | movable_object.debris                | 0.00000 |\r\n",
            "| 22    | static_object.bicycle_rack           | 0.00000 |\r\n",
            "+-------+--------------------------------------+---------+mAP : 5.702298929895424e-05\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| Index | Class name                           | AP      |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| 0     | human.pedestrian.adult               | 0.00000 |\r\n",
            "| 1     | human.pedestrian.child               | 0.00000 |\r\n",
            "| 2     | human.pedestrian.wheelchair          | 0.00000 |\r\n",
            "| 3     | human.pedestrian.stroller            | 0.00000 |\r\n",
            "| 4     | human.pedestrian.personal_mobility   | 0.00000 |\r\n",
            "| 5     | human.pedestrian.police_officer      | 0.00000 |\r\n",
            "| 6     | human.pedestrian.construction_worker | 0.00000 |\r\n",
            "| 7     | animal                               | 0.00000 |\r\n",
            "| 8     | vehicle.car                          | 0.00009 |\r\n",
            "| 9     | vehicle.motorcycle                   | 0.00000 |\r\n",
            "| 10    | vehicle.bicycle                      | 0.00000 |\r\n",
            "| 11    | vehicle.bus.bendy                    | 0.00000 |\r\n",
            "| 12    | vehicle.bus.rigid                    | 0.00000 |\r\n",
            "| 13    | vehicle.truck                        | 0.00024 |\r\n",
            "| 14    | vehicle.construction                 | 0.00000 |\r\n",
            "| 16    | vehicle.emergency.police             | 0.00000 |\r\n",
            "| 17    | vehicle.trailer                      | 0.00315 |\r\n",
            "| 18    | movable_object.barrier               | 0.00000 |\r\n",
            "| 19    | movable_object.trafficcone           | 0.00000 |\r\n",
            "| 20    | movable_object.pushable_pullable     | 0.00000 |\r\n",
            "| 21    | movable_object.debris                | 0.00000 |\r\n",
            "| 22    | static_object.bicycle_rack           | 0.00000 |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "mAP : 0.00015868305394661262\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| Index | Class name                           | AP      |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "| 0     | human.pedestrian.adult               | 0.00847 |\r\n",
            "| 1     | human.pedestrian.child               | 0.00000 |\r\n",
            "| 2     | human.pedestrian.wheelchair          | 0.00000 |\r\n",
            "| 3     | human.pedestrian.stroller            | 0.00000 |\r\n",
            "| 4     | human.pedestrian.personal_mobility   | 0.00000 |\r\n",
            "| 5     | human.pedestrian.police_officer      | 0.00000 |\r\n",
            "| 6     | human.pedestrian.construction_worker | 0.00000 |\r\n",
            "| 7     | animal                               | 0.00000 |\r\n",
            "| 8     | vehicle.car                          | 0.30474 |\r\n",
            "| 9     | vehicle.motorcycle                   | 0.05202 |\r\n",
            "| 10    | vehicle.bicycle                      | 0.00742 |\r\n",
            "| 11    | vehicle.bus.bendy                    | 0.00000 |\r\n",
            "| 12    | vehicle.bus.rigid                    | 0.00972 |\r\n",
            "| 13    | vehicle.truck                        | 0.09200 |\r\n",
            "| 14    | vehicle.construction                 | 0.03427 |\r\n",
            "| 16    | vehicle.emergency.police             | 0.00000 |\r\n",
            "| 17    | vehicle.trailer                      | 0.01856 |\r\n",
            "| 18    | movable_object.barrier               | 0.00036 |\r\n",
            "| 19    | movable_object.trafficcone           | 0.00566 |\r\n",
            "| 20    | movable_object.pushable_pullable     | 0.00154 |\r\n",
            "| 21    | movable_object.debris                | 0.00000 |\r\n",
            "| 22    | static_object.bicycle_rack           | 0.03604 |\r\n",
            "+-------+--------------------------------------+---------+\r\n",
            "mAP : 0.02594514751841035"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc-2Uf6MlQiE",
        "outputId": "8a0d0f3d-fe99-4cd8-9c21-052c54e1b2d1"
      },
      "source": [
        "precision, recall, AP, f1, ap_class = evaluate(\n",
        "    model,\n",
        "    test_samples,\n",
        "    iou_thres=0.5,\n",
        "    conf_thres=0.7,\n",
        "    nms_thres=0.07, \n",
        "    img_size=opt.img_size,\n",
        "    batch_size=2,\n",
        ")\n",
        "evaluation_metrics = [\n",
        "    (\"val_precision\", precision.mean()),\n",
        "    (\"val_recall\", recall.mean()),\n",
        "    (\"val_mAP\", AP.mean()),\n",
        "    (\"val_f1\", f1.mean()),\n",
        "]\n",
        "#logger.list_of_scalars_summary(evaluation_metrics, epoch)\n",
        "\n",
        "# Print class APs and mAP\n",
        "ap_table = [[\"Index\", \"Class name\", \"AP\"]]\n",
        "for i, c in enumerate(ap_class):\n",
        "    ap_table += [[c, class_names[c], \"%.5f\" % AP[i]]]\n",
        "print(AsciiTable(ap_table).table)\n",
        "print(f\"---- mAP {AP.mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detecting objects: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3330/3330 [1:51:23<00:00,  2.01s/it]\n",
            "Computing AP: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:00<00:00, 130.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-------+--------------------------------------+---------+\n",
            "| Index | Class name                           | AP      |\n",
            "+-------+--------------------------------------+---------+\n",
            "| 0     | human.pedestrian.adult               | 0.00847 |\n",
            "| 1     | human.pedestrian.child               | 0.00000 |\n",
            "| 2     | human.pedestrian.wheelchair          | 0.00000 |\n",
            "| 3     | human.pedestrian.stroller            | 0.00000 |\n",
            "| 4     | human.pedestrian.personal_mobility   | 0.00000 |\n",
            "| 5     | human.pedestrian.police_officer      | 0.00000 |\n",
            "| 6     | human.pedestrian.construction_worker | 0.00000 |\n",
            "| 7     | animal                               | 0.00000 |\n",
            "| 8     | vehicle.car                          | 0.30474 |\n",
            "| 9     | vehicle.motorcycle                   | 0.05202 |\n",
            "| 10    | vehicle.bicycle                      | 0.00742 |\n",
            "| 11    | vehicle.bus.bendy                    | 0.00000 |\n",
            "| 12    | vehicle.bus.rigid                    | 0.00972 |\n",
            "| 13    | vehicle.truck                        | 0.09200 |\n",
            "| 14    | vehicle.construction                 | 0.03427 |\n",
            "| 16    | vehicle.emergency.police             | 0.00000 |\n",
            "| 17    | vehicle.trailer                      | 0.01856 |\n",
            "| 18    | movable_object.barrier               | 0.00036 |\n",
            "| 19    | movable_object.trafficcone           | 0.00566 |\n",
            "| 20    | movable_object.pushable_pullable     | 0.00154 |\n",
            "| 21    | movable_object.debris                | 0.00000 |\n",
            "| 22    | static_object.bicycle_rack           | 0.03604 |\n",
            "+-------+--------------------------------------+---------+\n",
            "---- mAP 0.02594514751841035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eNn34W70xfj"
      },
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "  print(param_group['lr'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWea0HlsTB1a"
      },
      "source": [
        "img, tragets = next(iter(dataloader))\n",
        "img = Variable(img.to(device))\n",
        "model.eval()\n",
        "target = []\n",
        "for t in tragets:\n",
        "  if t[0] == 0:\n",
        "    target.append(t[2:])\n",
        "output = model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "kgRJieLM2qob",
        "outputId": "c5f1dcef-ceac-4e8d-e9d3-edfac77cdbce"
      },
      "source": [
        "out = non_max_suppression(output,conf_thres = 0.75,nms_thres = 0.1)\n",
        "out[0][:,:2] /=1024\n",
        "out[0][:,3:5] /=1024\n",
        "#print(loss)\n",
        "draw(target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "5jHwWqMFX8n5",
        "outputId": "188267ff-4e92-412a-ed61-d56ea132bfb6"
      },
      "source": [
        "draw(out[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "d2qIApnwh5pK",
        "outputId": "85b4d0e4-a79c-4b00-f0cf-2116b7fef96f"
      },
      "source": [
        "draw(out[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shkMUZFghdKy"
      },
      "source": [
        "dpi = 80\n",
        "im_data = img[0].cpu().permute(1, 2, 0)\n",
        "height, width, depth = im_data.shape\n",
        "\n",
        "# What size does the figure need to be in inches to fit the image?\n",
        "figsize = width / float(dpi), height / float(dpi)\n",
        "\n",
        "# Create a figure of the right size with one axes that takes up the full figure\n",
        "fig = plt.figure(figsize=figsize)\n",
        "ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "# Hide spines, ticks, etc.\n",
        "ax.axis('off')\n",
        "\n",
        "# Display the image.\n",
        "ax.imshow(im_data, cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M1c8A9sm6ny"
      },
      "source": [
        "!mv \"singledepthyolo(leastnoparameters-starting).pth\" checkpoints/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0jfQRuyqhHq"
      },
      "source": [
        "#Testing (Nuscene's Basics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1v635cuk858"
      },
      "source": [
        "classes = nusc.get('category',)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVYn959sH3Pz"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxTzVdt1HyNt"
      },
      "source": [
        "img, label = next(iter(dataloader))\n",
        "print(img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q89sfq4sHyPY"
      },
      "source": [
        "loss, out = model(img.to(device),label.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EL2lkUEUaeZ"
      },
      "source": [
        "print(non_max_suppression(out.to(device))[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6K8n4CZT7Ft"
      },
      "source": [
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-DD17QtR2dq"
      },
      "source": [
        "del imgs,targets,out,loss\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgzPg8rc4Bse"
      },
      "source": [
        "print(label[:,2].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rju84T256zm-"
      },
      "source": [
        "print(label[:,2].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3TwxDkCqj-g"
      },
      "source": [
        "%matplotlib inline\n",
        "from nuscenes.nuscenes import NuScenes \n",
        "\n",
        "nusc = NuScenes(version='v1.0-mini', dataroot='drive/My Drive/data', verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A13iNdjFqkm_"
      },
      "source": [
        "scene = nusc.scene[0]\n",
        "scene"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hkw-qStdrcmq"
      },
      "source": [
        "sample_token = scene['first_sample_token']\n",
        "sample = nusc.get('sample',sample_token)\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0WDSEtHOK0O"
      },
      "source": [
        "for first_anno in sample['anns'] :\n",
        "  anno = nusc.get('sample_annotation',first_anno)\n",
        "  vis = nusc.get('visibility',anno['visibility_token'])\n",
        "  print(vis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u035LGwgPRiA"
      },
      "source": [
        "vis = nusc.get('visibility',anno['visibility_token'])\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9b82vLGnJ4v"
      },
      "source": [
        "import json\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/data/v1.0-mini/category.json') as f:\n",
        "  data = json.load(f)\n",
        "for d in data:\n",
        "  print(d['name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SLzR8LenKIE"
      },
      "source": [
        "img,targets = iter(dataloader).next()\n",
        "plt.imshow(  img[0].permute(1, 2, 0)  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvAnhNUPhFgU"
      },
      "source": [
        "output = model(img.to(device))\n",
        "print(output[...,8].max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX05wMiohJNz"
      },
      "source": [
        "output = non_max_suppression(output,conf_thres=0.9)\n",
        "print(output[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw4rYBEDWKaY"
      },
      "source": [
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU7biujtWx1R"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib as mpl\n",
        "import matplotlib.patches as patches\n",
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcQwss6bXKoR"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111)\n",
        "for annotation in targets:\n",
        "  annotation = nusc.get('sample_annotation',annotation)\n",
        "  cordinate = [annotation['translation'][i] - ego_pose['translation'][i] for i in range(3)]\n",
        "  cordinate[0], cordinate[1] = rotate_around_point_lowperf(cordinate[:2], ego_yaw, origin=(0, 0))\n",
        "  rotation_yaw = quaternion_yaw(annotation['rotation']) - math.pi/2\n",
        "  print(\"rotation_yaw = \", quaternion_yaw(annotation['rotation']) + math.pi )\n",
        "  object_rotation = rotation_yaw - ego_yaw\n",
        "  height = annotation['size'][0]\n",
        "  width = annotation['size'][1]\n",
        "  x_temp, y_temp = rotate_around_point_lowperf((cordinate[0],cordinate[1]),2*math.pi -  (object_rotation+math.pi/2), origin=(cordinate[0]-width/2, cordinate[1] - height/2))\n",
        "  x_offset, y_offset = x_temp - cordinate[0], y_temp - cordinate[1]\n",
        "  rectas = patches.Rectangle(xy=((cordinate[0]-width/2) - x_offset, (cordinate[1] - height/2) - y_offset) ,width=width, angle = (object_rotation+math.pi/2)*180/math.pi, height=height, linewidth=1, color='blue', fill=False)\n",
        "  ax.add_patch(rectas)\n",
        "  ax.scatter(cordinate[0], cordinate[1], color = 'red', s=10)\n",
        "  break\n",
        "ax.scatter(0, 0)\n",
        "plt.xlim(-80,80)\n",
        "plt.ylim(-80,80)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xsHPdyHjeJ7"
      },
      "source": [
        "dataset.categories[18]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwvIc7iFZ__d"
      },
      "source": [
        "#Angle testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbhkMNF-aCGL"
      },
      "source": [
        "#30degree\n",
        "rotation_yaw = 0\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT11qiCtaLLS"
      },
      "source": [
        "#120degree\n",
        "rotation_yaw = math.pi/2 \n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjJ5mE2TaN6H"
      },
      "source": [
        "#210degree\n",
        "rotation_yaw = math.pi\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXNmm2SQavVU"
      },
      "source": [
        "#300degree\n",
        "rotation_yaw = 3*math.pi/2\n",
        "r1 = (1 + math.sin(rotation_yaw))/2\n",
        "r2 = (1 + math.cos(rotation_yaw))/2\n",
        "print(r1,r2)\n",
        "teta1 = math.asin(2*r1 - 1)\n",
        "teta2 = math.acos(2*r2 - 1)\n",
        "print(teta1,teta2)\n",
        "print(angle_decoder([r1,r2])*180/math.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TGwbnheayIx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_2iY7HwCTlm"
      },
      "source": [
        "#Intersection Area Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLZVErYZxX4D"
      },
      "source": [
        "from turfpy.transformation import intersect\n",
        "from turfpy.measurement import area\n",
        "from geojson import Feature\n",
        "f = Feature(geometry={\"coordinates\": [\n",
        "[[-122.801742, 45.48565], [-122.801742, 45.60491],\n",
        "[-122.584762, 45.60491], [-122.584762, 45.48565],\n",
        "[-122.801742, 45.48565]]], \"type\": \"Polygon\"})\n",
        "b = Feature(geometry={\"coordinates\": [\n",
        "[[-122.520217, 45.535693], [-122.64038, 45.553967],\n",
        "[-122.720031, 45.526554], [-122.669906, 45.507309],\n",
        "[-122.723464, 45.446643], [-122.532577, 45.408574],\n",
        "[-122.487258, 45.477466], [-122.520217, 45.535693]\n",
        "]], \"type\": \"Polygon\"})\n",
        "inter = intersect([f, b])\n",
        "area(inter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CePQxQ8Uxaye"
      },
      "source": [
        "!pip install turfpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HSDxXySxjqN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}